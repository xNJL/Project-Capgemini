{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X - HEC Embeddings 2 : Advanced Word Representations\n",
    "\n",
    "In this practical session, we will focus on word embeddings through word2vec and a simple classification model for sentiment analysis. Once a word2vec skipgram is trained, we can visualize learned word vectors in a reduced space and use them in our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_date_diner</th>\n",
       "      <th>review_has_answer</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_rating_value</th>\n",
       "      <th>review_rating_service</th>\n",
       "      <th>review_rating_atmosphere</th>\n",
       "      <th>review_rating_food</th>\n",
       "      <th>review_title</th>\n",
       "      <th>...</th>\n",
       "      <th>rest_rating_excellent</th>\n",
       "      <th>rest_rating_very_good</th>\n",
       "      <th>rest_rating_neutral</th>\n",
       "      <th>rest_rating_poor</th>\n",
       "      <th>rest_rating_terrible</th>\n",
       "      <th>rest_url</th>\n",
       "      <th>rest_url_menu</th>\n",
       "      <th>rest_adress</th>\n",
       "      <th>rest_description</th>\n",
       "      <th>grp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g191301-d4453079-r728219948</td>\n",
       "      <td>2019-11-22</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birthday Shots shots shots!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g191301-d4453079-r728632295</td>\n",
       "      <td>2019-11-24</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id review_date review_date_diner  \\\n",
       "0  g191301-d4453079-r728219948  2019-11-22        2019-11-01   \n",
       "1  g191301-d4453079-r728632295  2019-11-24        2019-11-01   \n",
       "\n",
       "   review_has_answer  review_rating  review_rating_value  \\\n",
       "0               True            4.5                  NaN   \n",
       "1               True            4.5                  NaN   \n",
       "\n",
       "   review_rating_service  review_rating_atmosphere  review_rating_food  \\\n",
       "0                    NaN                       NaN                 NaN   \n",
       "1                    NaN                       NaN                 NaN   \n",
       "\n",
       "                  review_title  ... rest_rating_excellent  \\\n",
       "0  Birthday Shots shots shots!  ...                   833   \n",
       "1                     Louis!!!  ...                   833   \n",
       "\n",
       "  rest_rating_very_good rest_rating_neutral rest_rating_poor  \\\n",
       "0                   101                43.0             34.0   \n",
       "1                   101                43.0             34.0   \n",
       "\n",
       "  rest_rating_terrible                                           rest_url  \\\n",
       "0                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "1                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "\n",
       "                                       rest_url_menu         rest_adress  \\\n",
       "0  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "1  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "\n",
       "                                    rest_description  grp  \n",
       "0  Perched on the Thames riverside, this beautifu...  cap  \n",
       "1  Perched on the Thames riverside, this beautifu...  cap  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data path\n",
    "file_path = os.getcwd() + '/clean_full_graph.csv.gzip'\n",
    "\n",
    "# Read csv file with right parameters\n",
    "df_all = pd.read_csv(file_path, \n",
    "                     compression='gzip', \n",
    "                     low_memory=False, \n",
    "                     parse_dates=['review_date', 'review_date_diner'])\n",
    "\n",
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current exercice we'll work with Capgemini Invent's dataset, so that everyone has the same data. Later on you could try to do your own embedding with your scrapped data and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap = df_all[df_all.grp == 'cap'].reset_index(drop=True)\n",
    "df_cap.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap = df_cap[df_cap['review_content'].str.len() >= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Text Encoding\n",
    "This part concerns tokenization and text encoding with TensorFlow modules :\n",
    "\n",
    "*(i) Build the token vocabulary* <br>\n",
    "*(ii) Build a text encoder relying each word to an index, and thus each text to a sequence of word indices* (```list```) <br>\n",
    "*(iii) Build a TensorFlow dataset for word2vec training*\n",
    "\n",
    "\n",
    "1. **Tokenization** : Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 10000\n",
    "\n",
    "df_cap['review_content'] = df_cap['review_content'].apply(lambda x : literal_eval(x)[0])\n",
    "\n",
    "reviews = df_cap['review_content'][:DATASET_SIZE].values.tolist()\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "vocabulary_set = set()\n",
    "\n",
    "for text in tqdm_notebook(reviews) :\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    vocabulary_set.update(tokens)\n",
    "    \n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Token Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
    "\n",
    "print(df_cap['review_content'][0])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "token_encoded_text = token_encoder.encode(df_cap['review_content'][0])\n",
    "print(token_encoded_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "token_decoded_text = token_encoder.decode(token_encoded_text)\n",
    "print(token_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tk in token_encoded_text[:10] :\n",
    "    \n",
    "    print('{} ----> {}'.format(tk, token_encoder.decode([tk])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Build Learning Dataset**\n",
    "\n",
    "To learn word2vec vectors, we define center and context words. Thus, we concatenate each document, i.e. sequence of word indices to make the moving context window possible.\n",
    "\n",
    "*Question : build this corpus of concatenated encoded documents, i.e. transform a list of* ```DATASET_SIZE``` *lists of indices in one list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #### FILL THE BLANK(S) ####\n",
    "all_tokens = \n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.keras.preprocessing.sequence.make_sampling_table``` and ```tf.keras.preprocessing.sequence.skipgrams``` sequence preprocessing functions are useful to build the negative sampled dataset. The ```skipgrams```function allows to browse the entire document (```all_tokens```) to identify positive and negative pairs of center and context words.\n",
    "\n",
    "*Question : use these functions to get skipgram word pairs with labels (positive or negative). You can use the default* ```sampling_factor```, ```window_size```=4 *and generate one* ```negative_samples```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "\n",
    "sampling_table = \n",
    "\n",
    "#### FILL THE BLANK(S) ####\n",
    "X, y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word2vec training, we won't use a validation dataset this time. In eventual further experimentations, you could include it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X)\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# threshold_set = int(np.floor(0.8*len(X)))\n",
    "# \n",
    "# X_train, y_train = tf.convert_to_tensor(X[:threshold_set], dtype=tf.int32), tf.convert_to_tensor(y[:threshold_set], dtype=tf.int32)\n",
    "# center_words_train, context_words_train = X_train[:,0], X_train[:,1]\n",
    "# \n",
    "# word2vec_dataset_train = tf.data.Dataset.from_tensor_slices((center_words_train, context_words_train, y_train)).batch(BATCH_SIZE)\n",
    "# \n",
    "# X_test, y_test = tf.convert_to_tensor(X[threshold_set:], dtype=tf.int32), tf.convert_to_tensor(y[threshold_set:], dtype=tf.int32)\n",
    "# center_words_test, context_words_test = X_test[:,0], X_test[:,1]\n",
    "# \n",
    "# word2vec_dataset_test = tf.data.Dataset.from_tensor_slices((center_words_test, context_words_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "X, y = tf.convert_to_tensor(X, dtype=tf.int32), tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "center_words, context_words = X[:,0], X[:,1]\n",
    "\n",
    "word2vec_dataset = tf.data.Dataset.from_tensor_slices((center_words, context_words, y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec SkipGram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_words_batch, context_words_batch, y_batch = next(iter(word2vec_dataset))\n",
    "print(center_words_batch.shape, context_words_batch.shape, y_batch.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **SkipGram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, d_model, token_vocab_size):\n",
    "        \n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = token_vocab_size\n",
    "        \n",
    "        self.input_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model, name='input_embedding')\n",
    "        self.output_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model, name='output_embedding')\n",
    "        \n",
    "        \n",
    "    def call(self, center_word, context_word):\n",
    "        \n",
    "        center_vector = self.input_embedding(center_word)\n",
    "        context_vector = self.output_embedding(context_word)\n",
    "        \n",
    "        dot_product = tf.math.reduce_sum(tf.multiply(center_vector, context_vector), axis=1)\n",
    "        \n",
    "        return tf.math.sigmoid(dot_product) # loss : from_logits=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram = SkipGram(300, token_encoder.vocab_size)\n",
    "assert skipgram(center_words_batch, context_words_batch).shape[0] == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Optimization Objective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "# test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(center_word, context_word, label):\n",
    "    \n",
    "    with tf.GradientTape() as tape :\n",
    "        \n",
    "        prediction = skipgram(center_word, context_word)\n",
    "        loss = loss_object(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, skipgram.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, skipgram.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_step(center_word, context_word, label):\n",
    "#     \n",
    "#     prediction = skipgram(center_word, context_word)\n",
    "#     loss = loss_object(label, prediction)\n",
    "# \n",
    "#     test_loss(loss)\n",
    "#     test_accuracy(label, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch can take almost 10 minutes so define a small number of epochs to end the hands on !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### FILL THE BLANK(S) ####\n",
    "EPOCHS = \n",
    "\n",
    "DATASET_LENGTH = X.shape[0] # threshold_set\n",
    "\n",
    "\n",
    "for epoch in tqdm_notebook(iterable = range(EPOCHS), total = EPOCHS, desc = 'T R A I N I N G') :\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    # test_loss.reset_states()\n",
    "    # test_accuracy.reset_states()\n",
    "    \n",
    "    for center_word, context_word, label in tqdm_notebook(word2vec_dataset, \n",
    "                                                          total = int(np.ceil(DATASET_LENGTH/BATCH_SIZE)), \n",
    "                                                          desc = 'Epoch {}/{}'.format(epoch+1, EPOCHS)) : \n",
    "        \n",
    "        train_step(center_word, context_word, label)\n",
    "    \n",
    "    \n",
    "    # for center_word, context_word, label in word2vec_dataset_test :\n",
    "    #     \n",
    "    #     test_step(center_word, context_word, label)\n",
    "    \n",
    "\n",
    "    print ('Loss : {} - Accuracy : {}\\n'.format(train_loss.result(), \n",
    "                                                train_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipgram.save_weights(\"'./checkpoints/skipgram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions : get the weights of the* ```input_embedding``` *layer and store them in a variable* ```weights```. *These weights correspond to word2vec skipgram embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = #### FILL THE BLANK(S) ####\n",
    "\n",
    "print(weights, weights.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question : use the following cell to store the learned vectors in the correct format and visualize them in the Embedding Projector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for idx, word in enumerate(token_encoder.tokens):\n",
    "    \n",
    "    vec = weights[idx+1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize your trained embeddings, open the [Embedding Projector](http://projector.tensorflow.org/) :\n",
    "\n",
    "- Click on \"Load data\".\n",
    "- Upload the two files we created above : vecs.tsv and meta.tsv.\n",
    "\n",
    "The embeddings will now be displayed. You can search for words from ```token_encoder.tokens``` to find their closest neighbors.\n",
    "\n",
    "# Text Classication : Sentiment Analysis\n",
    "Our learned word embeddings can be used to represent the words of a text and to build a text representation. This text representation will be useful for classifcation.\n",
    "\n",
    "*Question : in this part, our objective is to train a text classification model for sentiment analysis using the first* ```DATASET_SIZE``` reviews*. You have to :*\n",
    "1. *Build the learning dataset : use the ```review_content``` column for text inputs and ```rest_rating``` for sentiments (outputs).* **NB** *: Don't forget to encode and pad your text, and use one-hot encoding for sentiment labels.*\n",
    "2. *Once the learning dataset built, you have to split it into train and validation datasets.*\n",
    "3. *Define and train a text classification model.*\n",
    "\n",
    "As indications, you can use ```tf.keras.preprocessing.sequence.pad_sequences``` for text padding and ```tf.keras.utils.to_categorical``` for one-hot label encoding. For your first model, you can already use the simple text classification model described in the course with ```tf.keras.Sequential``` API :\n",
    "1. An embedding layer : if you use zero padding, you can set ```mask_zero=True``` and use the ```weights``` matrix for initialization.\n",
    "2. A dense layer without particular activation function for linear projection of the previous embedding vectors.\n",
    "3. A global average pooling (1D).\n",
    "4. A final dense layer for linear projection in a $d$-dimensional space for sentiment prediction, with $d$ the number of possible sentiments/classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FILL THE BLANKS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve your sentiment model coupling reviewer embeddings and word embeddigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"classif.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewer classification model \n",
    "Create a second sentiment model that will solely use the reviewer embeddings to predict the restaurant rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "n2v = Word2Vec.load(\"node2vec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_documents = df_cap[['review_content', 'rest_rating', 'reviewer_pseudo']].head(DATASET_SIZE).dropna().reset_index(drop=True)\n",
    "labeled_documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a second sentiment model that will solely use the reviewer embeddings to predict the sentiment\n",
    "2. Merge the first and two models to complete the above architecture. \n",
    "\n",
    "\n",
    "You will be using the functional API of keras. \n",
    "\n",
    "#### Proposed architecture \n",
    "**Review NLP model**  \n",
    "1. Input Shape : the input text sequence of max size N (including padding)\n",
    "2. Embedding Layers: mapping the embedding matrix with your input sequence\n",
    "3. Dense layer: linear projection of the previous embedding\n",
    "3. Global average Pooling 1D: \n",
    "4. Flatten: just flattening the representation into a vector, this is the review embedding ! \n",
    "\n",
    "**Reviewer (meta features) model**\n",
    "5. Input Shape : the input reviewer embedding of size N' (dimension you choose for node2vec)\n",
    "\n",
    "\n",
    "**Merging the two models**\n",
    "6. Concatenate layer merging the two models input. \n",
    "7. Global average Pooling 1D: \n",
    "8. Flatten: Final representation before classification\n",
    "9. Dense layer with softmax activation of size corresponding to the number of class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(400, input_shape=(64,)),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "sentiment_model2.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = sentiment_model2.fit(\n",
    "    X2_train, y2_train, epochs=10, validation_data=(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True),\n",
    "    tf.keras.layers.Dense(400),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last'),\n",
    "    tf.keras.layers.Dense(400, input_shape=(64,)),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "sentiment_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "n_classes = 5\n",
    "\n",
    "input_1 = tf.keras.layers.Input(shape=(743,))\n",
    "emb = tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True)(input_1)\n",
    "dense = tf.keras.layers.Dense(400)(emb)\n",
    "pool = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(dense)\n",
    "flatten = tf.keras.layers.Flatten()(pool)\n",
    "tf.keras.layers.Dense(5)\n",
    "\n",
    "input_2 = tf.keras.layers.Input(shape=(64,))\n",
    "\n",
    "\n",
    "# Concatenate\n",
    "concat = tf.keras.layers.Concatenate()([flatten, input_2])\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(400)(concat)\n",
    "\n",
    "# output layer\n",
    "output = tf.keras.layers.Dense(units=n_classes,\n",
    "                               activation=tf.keras.activations.softmax)(dense_2)\n",
    "    \n",
    "full_model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output])\n",
    "\n",
    "full_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "print(full_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.shape, X2_train.shape, y_train.shape, y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = full_model.fit([X_train, X2_train], y_train, epochs=10, validation_data=([X_test, X2_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
