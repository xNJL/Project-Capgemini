{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X - HEC Embeddings 2 : Advanced Word Representations\n",
    "\n",
    "In this practical session, we will focus on word embeddings through word2vec and a simple classification model for sentiment analysis. Once a word2vec skipgram is trained, we can visualize learned word vectors in a reduced space and use them in our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:04.818397Z",
     "start_time": "2020-03-08T09:31:55.515479Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:09.989272Z",
     "start_time": "2020-03-08T09:32:04.820866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_date_diner</th>\n",
       "      <th>review_has_answer</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_rating_value</th>\n",
       "      <th>review_rating_service</th>\n",
       "      <th>review_rating_atmosphere</th>\n",
       "      <th>review_rating_food</th>\n",
       "      <th>review_title</th>\n",
       "      <th>...</th>\n",
       "      <th>rest_rating_excellent</th>\n",
       "      <th>rest_rating_very_good</th>\n",
       "      <th>rest_rating_neutral</th>\n",
       "      <th>rest_rating_poor</th>\n",
       "      <th>rest_rating_terrible</th>\n",
       "      <th>rest_url</th>\n",
       "      <th>rest_url_menu</th>\n",
       "      <th>rest_adress</th>\n",
       "      <th>rest_description</th>\n",
       "      <th>grp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>g191301-d4453079-r728219948</td>\n",
       "      <td>2019-11-22</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birthday Shots shots shots!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>g191301-d4453079-r728632295</td>\n",
       "      <td>2019-11-24</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id review_date review_date_diner  \\\n",
       "0  g191301-d4453079-r728219948  2019-11-22        2019-11-01   \n",
       "1  g191301-d4453079-r728632295  2019-11-24        2019-11-01   \n",
       "\n",
       "   review_has_answer  review_rating  review_rating_value  \\\n",
       "0               True            4.5                  NaN   \n",
       "1               True            4.5                  NaN   \n",
       "\n",
       "   review_rating_service  review_rating_atmosphere  review_rating_food  \\\n",
       "0                    NaN                       NaN                 NaN   \n",
       "1                    NaN                       NaN                 NaN   \n",
       "\n",
       "                  review_title  ... rest_rating_excellent  \\\n",
       "0  Birthday Shots shots shots!  ...                   833   \n",
       "1                     Louis!!!  ...                   833   \n",
       "\n",
       "  rest_rating_very_good rest_rating_neutral rest_rating_poor  \\\n",
       "0                   101                43.0             34.0   \n",
       "1                   101                43.0             34.0   \n",
       "\n",
       "  rest_rating_terrible                                           rest_url  \\\n",
       "0                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "1                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "\n",
       "                                       rest_url_menu         rest_adress  \\\n",
       "0  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "1  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "\n",
       "                                    rest_description  grp  \n",
       "0  Perched on the Thames riverside, this beautifu...  cap  \n",
       "1  Perched on the Thames riverside, this beautifu...  cap  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data path\n",
    "file_path = os.getcwd() + '/clean_full_graph.csv.gzip'\n",
    "\n",
    "# Read csv file with right parameters\n",
    "df_all = pd.read_csv(file_path, \n",
    "                     compression='gzip', \n",
    "                     low_memory=False, \n",
    "                     parse_dates=['review_date', 'review_date_diner'])\n",
    "\n",
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current exercice we'll work with Capgemini Invent's dataset, so that everyone has the same data. Later on you could try to do your own embedding with your scrapped data and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:10.251052Z",
     "start_time": "2020-03-08T09:32:09.992202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_date_diner</th>\n",
       "      <th>review_has_answer</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_rating_value</th>\n",
       "      <th>review_rating_service</th>\n",
       "      <th>review_rating_atmosphere</th>\n",
       "      <th>review_rating_food</th>\n",
       "      <th>review_title</th>\n",
       "      <th>...</th>\n",
       "      <th>rest_rating_excellent</th>\n",
       "      <th>rest_rating_very_good</th>\n",
       "      <th>rest_rating_neutral</th>\n",
       "      <th>rest_rating_poor</th>\n",
       "      <th>rest_rating_terrible</th>\n",
       "      <th>rest_url</th>\n",
       "      <th>rest_url_menu</th>\n",
       "      <th>rest_adress</th>\n",
       "      <th>rest_description</th>\n",
       "      <th>grp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>g191301-d4453079-r728219948</td>\n",
       "      <td>2019-11-22</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birthday Shots shots shots!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>g191301-d4453079-r728632295</td>\n",
       "      <td>2019-11-24</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id review_date review_date_diner  \\\n",
       "0  g191301-d4453079-r728219948  2019-11-22        2019-11-01   \n",
       "1  g191301-d4453079-r728632295  2019-11-24        2019-11-01   \n",
       "\n",
       "   review_has_answer  review_rating  review_rating_value  \\\n",
       "0               True            4.5                  NaN   \n",
       "1               True            4.5                  NaN   \n",
       "\n",
       "   review_rating_service  review_rating_atmosphere  review_rating_food  \\\n",
       "0                    NaN                       NaN                 NaN   \n",
       "1                    NaN                       NaN                 NaN   \n",
       "\n",
       "                  review_title  ... rest_rating_excellent  \\\n",
       "0  Birthday Shots shots shots!  ...                   833   \n",
       "1                     Louis!!!  ...                   833   \n",
       "\n",
       "  rest_rating_very_good rest_rating_neutral rest_rating_poor  \\\n",
       "0                   101                43.0             34.0   \n",
       "1                   101                43.0             34.0   \n",
       "\n",
       "  rest_rating_terrible                                           rest_url  \\\n",
       "0                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "1                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "\n",
       "                                       rest_url_menu         rest_adress  \\\n",
       "0  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "1  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "\n",
       "                                    rest_description  grp  \n",
       "0  Perched on the Thames riverside, this beautifu...  cap  \n",
       "1  Perched on the Thames riverside, this beautifu...  cap  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cap = df_all[df_all.grp == 'cap'].reset_index(drop=True)\n",
    "df_cap.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:10.448198Z",
     "start_time": "2020-03-08T09:32:10.257463Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cap = df_cap[df_cap['review_content'].str.len() >= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Text Encoding\n",
    "This part concerns tokenization and text encoding with TensorFlow modules :\n",
    "\n",
    "*(i) Build the token vocabulary* <br>\n",
    "*(ii) Build a text encoder relying each word to an index, and thus each text to a sequence of word indices* (```list```) <br>\n",
    "*(iii) Build a TensorFlow dataset for word2vec training*\n",
    "\n",
    "\n",
    "1. **Tokenization** : Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:13.079722Z",
     "start_time": "2020-03-08T09:32:10.450391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb71182b56c49a6adf8f7ec99d208a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18557"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SIZE = 10000\n",
    "\n",
    "df_cap['review_content'] = df_cap['review_content'].apply(lambda x : literal_eval(x)[0])\n",
    "\n",
    "reviews = df_cap['review_content'][:DATASET_SIZE].values.tolist()\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "vocabulary_set = set()\n",
    "\n",
    "for text in tqdm_notebook(reviews) :\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    vocabulary_set.update(tokens)\n",
    "    \n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Token Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:13.129665Z",
     "start_time": "2020-03-08T09:32:13.081980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The atmosphere in here is great, we came for birthday drinks and never left, music is on point too, it’s fun and lively. Lewis was super friendly and helpful serving us and even suggested some great tasting shots! Ask for Lewis when you visit!!\n",
      "\n",
      "\n",
      "[9055, 12798, 812, 18133, 6183, 345, 18000, 11442, 15031, 10886, 14538, 14354, 13745, 3946, 5006, 6183, 13579, 4471, 7113, 3, 12318, 119, 14354, 344, 4741, 7686, 13481, 3166, 14354, 4927, 1644, 9828, 14354, 11515, 17307, 17364, 345, 11968, 5474, 17967, 15031, 4741, 7793, 786, 5298]\n",
      "\n",
      "\n",
      "The atmosphere in here is great we came for birthday drinks and never left music is on point too it s fun and lively Lewis was super friendly and helpful serving us and even suggested some great tasting shots Ask for Lewis when you visit\n"
     ]
    }
   ],
   "source": [
    "token_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
    "\n",
    "print(df_cap['review_content'][0])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "token_encoded_text = token_encoder.encode(df_cap['review_content'][0])\n",
    "print(token_encoded_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "token_decoded_text = token_encoder.decode(token_encoded_text)\n",
    "print(token_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:13.154983Z",
     "start_time": "2020-03-08T09:32:13.146903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9055 ----> The\n",
      "12798 ----> atmosphere\n",
      "812 ----> in\n",
      "18133 ----> here\n",
      "6183 ----> is\n",
      "345 ----> great\n",
      "18000 ----> we\n",
      "11442 ----> came\n",
      "15031 ----> for\n",
      "10886 ----> birthday\n"
     ]
    }
   ],
   "source": [
    "for tk in token_encoded_text[:10] :\n",
    "    \n",
    "    print('{} ----> {}'.format(tk, token_encoder.decode([tk])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Build Learning Dataset**\n",
    "\n",
    "To learn word2vec vectors, we define center and context words. Thus, we concatenate each document, i.e. sequence of word indices to make the moving context window possible.\n",
    "\n",
    "*Question : build this corpus of concatenated encoded documents, i.e. transform a list of* ```DATASET_SIZE``` *lists of indices in one list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:36:49.043376Z",
     "start_time": "2020-03-08T09:36:48.372008Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences_reviews = [token_encoder.encode(review) for review in reviews]\n",
    "sequences_reviews_flat = [item for sublist in sequences_reviews for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:36:49.131196Z",
     "start_time": "2020-03-08T09:36:49.125113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #### FILL THE BLANK(S) ####\n",
    "all_tokens = sequences_reviews_flat\n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.keras.preprocessing.sequence.make_sampling_table``` and ```tf.keras.preprocessing.sequence.skipgrams``` sequence preprocessing functions are useful to build the negative sampled dataset. The ```skipgrams```function allows to browse the entire document (```all_tokens```) to identify positive and negative pairs of center and context words.\n",
    "\n",
    "*Question : use these functions to get skipgram word pairs with labels (positive or negative). You can use the default* ```sampling_factor```, ```window_size```=4 *and generate one* ```negative_samples```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:37:33.625560Z",
     "start_time": "2020-03-08T09:37:33.605506Z"
    }
   },
   "outputs": [],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(token_encoder.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:00.810862Z",
     "start_time": "2020-03-08T09:37:33.739690Z"
    }
   },
   "outputs": [],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "X, y = tf.keras.preprocessing.sequence.skipgrams(all_tokens, token_encoder.vocab_size, window_size=4, sampling_table=sampling_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:00.843562Z",
     "start_time": "2020-03-08T09:38:00.813893Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11062, 15876], [11606, 1850], [4077, 4077], [9055, 16861], [15887, 17670]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:00.897116Z",
     "start_time": "2020-03-08T09:38:00.874800Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word2vec training, we won't use a validation dataset this time. In eventual further experimentations, you could include it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:19.696968Z",
     "start_time": "2020-03-08T09:38:00.902412Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X)\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# threshold_set = int(np.floor(0.8*len(X)))\n",
    "# \n",
    "# X_train, y_train = tf.convert_to_tensor(X[:threshold_set], dtype=tf.int32), tf.convert_to_tensor(y[:threshold_set], dtype=tf.int32)\n",
    "# center_words_train, context_words_train = X_train[:,0], X_train[:,1]\n",
    "# \n",
    "# word2vec_dataset_train = tf.data.Dataset.from_tensor_slices((center_words_train, context_words_train, y_train)).batch(BATCH_SIZE)\n",
    "# \n",
    "# X_test, y_test = tf.convert_to_tensor(X[threshold_set:], dtype=tf.int32), tf.convert_to_tensor(y[threshold_set:], dtype=tf.int32)\n",
    "# center_words_test, context_words_test = X_test[:,0], X_test[:,1]\n",
    "# \n",
    "# word2vec_dataset_test = tf.data.Dataset.from_tensor_slices((center_words_test, context_words_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "X, y = tf.convert_to_tensor(X, dtype=tf.int32), tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "center_words, context_words = X[:,0], X[:,1]\n",
    "\n",
    "word2vec_dataset = tf.data.Dataset.from_tensor_slices((center_words, context_words, y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec SkipGram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.234409Z",
     "start_time": "2020-03-08T09:38:19.699300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(2048,)\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "center_words_batch, context_words_batch, y_batch = next(iter(word2vec_dataset))\n",
    "print(center_words_batch.shape, context_words_batch.shape, y_batch.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **SkipGram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.246688Z",
     "start_time": "2020-03-08T09:38:36.237037Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, d_model, token_vocab_size):\n",
    "        \n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = token_vocab_size\n",
    "        \n",
    "        self.input_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model, name='input_embedding')\n",
    "        self.output_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model, name='output_embedding')\n",
    "        \n",
    "        \n",
    "    def call(self, center_word, context_word):\n",
    "        \n",
    "        center_vector = self.input_embedding(center_word)\n",
    "        context_vector = self.output_embedding(context_word)\n",
    "        \n",
    "        dot_product = tf.math.reduce_sum(tf.multiply(center_vector, context_vector), axis=1)\n",
    "        \n",
    "        return tf.math.sigmoid(dot_product) # loss : from_logits=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.337516Z",
     "start_time": "2020-03-08T09:38:36.249059Z"
    }
   },
   "outputs": [],
   "source": [
    "skipgram = SkipGram(300, token_encoder.vocab_size)\n",
    "assert skipgram(center_words_batch, context_words_batch).shape[0] == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Optimization Objective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.348907Z",
     "start_time": "2020-03-08T09:38:36.343694Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.368622Z",
     "start_time": "2020-03-08T09:38:36.354963Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "# test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.380269Z",
     "start_time": "2020-03-08T09:38:36.373351Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(center_word, context_word, label):\n",
    "    \n",
    "    with tf.GradientTape() as tape :\n",
    "        \n",
    "        prediction = skipgram(center_word, context_word)\n",
    "        loss = loss_object(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, skipgram.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, skipgram.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.385458Z",
     "start_time": "2020-03-08T09:38:36.382476Z"
    }
   },
   "outputs": [],
   "source": [
    "# def test_step(center_word, context_word, label):\n",
    "#     \n",
    "#     prediction = skipgram(center_word, context_word)\n",
    "#     loss = loss_object(label, prediction)\n",
    "# \n",
    "#     test_loss(loss)\n",
    "#     test_accuracy(label, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch can take almost 10 minutes so define a small number of epochs to end the hands on !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:55:10.834992Z",
     "start_time": "2020-03-08T09:38:36.387695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992149c7e3c54962a07da5993dd85318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='T R A I N I N G', max=1.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d445b6c8b3734b40a484296a4ae94a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1/1', max=3618.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss : 0.283454954624176 - Accuracy : 89.18569946289062\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### FILL THE BLANK(S) ####\n",
    "EPOCHS = 1\n",
    "\n",
    "DATASET_LENGTH = X.shape[0] # threshold_set\n",
    "\n",
    "\n",
    "for epoch in tqdm_notebook(iterable = range(EPOCHS), total = EPOCHS, desc = 'T R A I N I N G') :\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    # test_loss.reset_states()\n",
    "    # test_accuracy.reset_states()\n",
    "    \n",
    "    for center_word, context_word, label in tqdm_notebook(word2vec_dataset, \n",
    "                                                          total = int(np.ceil(DATASET_LENGTH/BATCH_SIZE)), \n",
    "                                                          desc = 'Epoch {}/{}'.format(epoch+1, EPOCHS)) : \n",
    "        \n",
    "        train_step(center_word, context_word, label)\n",
    "    \n",
    "    \n",
    "    # for center_word, context_word, label in word2vec_dataset_test :\n",
    "    #     \n",
    "    #     test_step(center_word, context_word, label)\n",
    "    \n",
    "\n",
    "    print ('Loss : {} - Accuracy : {}\\n'.format(train_loss.result(), \n",
    "                                                train_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:55:10.840467Z",
     "start_time": "2020-03-08T09:55:10.837772Z"
    }
   },
   "outputs": [],
   "source": [
    "#skipgram.save_weights(\"'./checkpoints/skipgram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:55:10.848195Z",
     "start_time": "2020-03-08T09:55:10.843721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_embedding (Embedding)  multiple                  5567700   \n",
      "_________________________________________________________________\n",
      "output_embedding (Embedding) multiple                  5567700   \n",
      "=================================================================\n",
      "Total params: 11,135,400\n",
      "Trainable params: 11,135,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions : get the weights of the* ```input_embedding``` *layer and store them in a variable* ```weights```. *These weights correspond to word2vec skipgram embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T10:40:32.453541Z",
     "start_time": "2020-03-08T10:40:32.410388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02131558 -0.02155788 -0.04156015 ... -0.04552072  0.03853877\n",
      "   0.02474138]\n",
      " [-0.04875128 -0.01691667  0.00053961 ...  0.0203446  -0.00390484\n",
      "  -0.00934631]\n",
      " [-0.01243948  0.03444103 -0.04653388 ... -0.00194506 -0.02400517\n",
      "   0.04516551]\n",
      " ...\n",
      " [ 0.13041642  0.10881758 -0.04384284 ...  0.03644573 -0.04017317\n",
      "  -0.07953594]\n",
      " [ 0.10931685  0.06933687 -0.08837859 ...  0.10071741 -0.08837403\n",
      "  -0.03497447]\n",
      " [ 0.01763554  0.00616548  0.0224768  ... -0.02774796  0.00172073\n",
      "  -0.02844714]]\n",
      "(18559, 300)\n"
     ]
    }
   ],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "weights = (skipgram.get_weights()[0])\n",
    "\n",
    "print(weights, weights.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question : use the following cell to store the learned vectors in the correct format and visualize them in the Embedding Projector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T10:42:59.082130Z",
     "start_time": "2020-03-08T10:42:55.140959Z"
    }
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for idx, word in enumerate(token_encoder.tokens):\n",
    "    \n",
    "    vec = weights[idx+1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize your trained embeddings, open the [Embedding Projector](http://projector.tensorflow.org/) :\n",
    "\n",
    "- Click on \"Load data\".\n",
    "- Upload the two files we created above : vecs.tsv and meta.tsv.\n",
    "\n",
    "The embeddings will now be displayed. You can search for words from ```token_encoder.tokens``` to find their closest neighbors.\n",
    "\n",
    "# Text Classication : Sentiment Analysis\n",
    "Our learned word embeddings can be used to represent the words of a text and to build a text representation. This text representation will be useful for classifcation.\n",
    "\n",
    "*Question : in this part, our objective is to train a text classification model for sentiment analysis using the first* ```DATASET_SIZE``` reviews*. You have to :*\n",
    "1. *Build the learning dataset : use the ```review_content``` column for text inputs and ```rest_rating``` for sentiments (outputs).* **NB** *: Don't forget to encode and pad your text, and use one-hot encoding for sentiment labels.*\n",
    "2. *Once the learning dataset built, you have to split it into train and validation datasets.*\n",
    "3. *Define and train a text classification model.*\n",
    "\n",
    "As indications, you can use ```tf.keras.preprocessing.sequence.pad_sequences``` for text padding and ```tf.keras.utils.to_categorical``` for one-hot label encoding. For your first model, you can already use the simple text classification model described in the course with ```tf.keras.Sequential``` API :\n",
    "1. An embedding layer : if you use zero padding, you can set ```mask_zero=True``` and use the ```weights``` matrix for initialization.\n",
    "2. A dense layer without particular activation function for linear projection of the previous embedding vectors.\n",
    "3. A global average pooling (1D).\n",
    "4. A final dense layer for linear projection in a $d$-dimensional space for sentiment prediction, with $d$ the number of possible sentiments/classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:30.628000Z",
     "start_time": "2020-03-08T11:39:30.618107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 3.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df_cap['rest_rating'][:DATASET_SIZE]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:31.101331Z",
     "start_time": "2020-03-08T11:39:30.972295Z"
    }
   },
   "outputs": [],
   "source": [
    "##### FILL THE BLANKS\n",
    "#If we want to have 5 classes for the 5 rating we need to round it\n",
    "#to_categorial starts from 0 so it creates an extra class if there is no 0 \n",
    "#therefore we did \"-3\" so that the ratings start from 0 \n",
    "rest_rating_from0 = round(df_cap['rest_rating'][:DATASET_SIZE]-3)\n",
    "sentiment = tf.keras.utils.to_categorical(rest_rating_from0, num_classes=5)\n",
    "content_seq = tf.keras.preprocessing.sequence.pad_sequences(sequences_reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(content_seq, sentiment, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:31.598454Z",
     "start_time": "2020-03-08T11:39:31.593099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 608), (10000, 5))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_seq.shape, sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:32.296600Z",
     "start_time": "2020-03-08T11:39:32.156224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 300)         5567700   \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, None, 400)         120400    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 5)                 2005      \n",
      "=================================================================\n",
      "Total params: 5,690,105\n",
      "Trainable params: 5,690,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##### FILL THE BLANKS\n",
    "\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True),\n",
    "    tf.keras.layers.Dense(400),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last'),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:45:58.882260Z",
     "start_time": "2020-03-08T11:39:33.061695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/10\n",
      "6700/6700 [==============================] - 45s 7ms/sample - loss: 1.0492 - accuracy: 0.5399 - val_loss: 0.9683 - val_accuracy: 0.5573\n",
      "Epoch 2/10\n",
      "6700/6700 [==============================] - 38s 6ms/sample - loss: 0.9716 - accuracy: 0.5466 - val_loss: 0.9553 - val_accuracy: 0.5573\n",
      "Epoch 3/10\n",
      "6700/6700 [==============================] - 43s 6ms/sample - loss: 0.9585 - accuracy: 0.5466 - val_loss: 0.9450 - val_accuracy: 0.5573\n",
      "Epoch 4/10\n",
      "6700/6700 [==============================] - 36s 5ms/sample - loss: 0.9429 - accuracy: 0.5473 - val_loss: 0.9405 - val_accuracy: 0.5585\n",
      "Epoch 5/10\n",
      "6700/6700 [==============================] - 38s 6ms/sample - loss: 0.9228 - accuracy: 0.5499 - val_loss: 0.9200 - val_accuracy: 0.5576\n",
      "Epoch 6/10\n",
      "6700/6700 [==============================] - 36s 5ms/sample - loss: 0.8956 - accuracy: 0.5646 - val_loss: 0.8984 - val_accuracy: 0.5673\n",
      "Epoch 7/10\n",
      "6700/6700 [==============================] - 36s 5ms/sample - loss: 0.8558 - accuracy: 0.5828 - val_loss: 0.8822 - val_accuracy: 0.5945\n",
      "Epoch 8/10\n",
      "6700/6700 [==============================] - 36s 5ms/sample - loss: 0.8111 - accuracy: 0.6293 - val_loss: 0.8548 - val_accuracy: 0.5930\n",
      "Epoch 9/10\n",
      "6700/6700 [==============================] - 39s 6ms/sample - loss: 0.7629 - accuracy: 0.6622 - val_loss: 0.8392 - val_accuracy: 0.6142\n",
      "Epoch 10/10\n",
      "6700/6700 [==============================] - 39s 6ms/sample - loss: 0.7154 - accuracy: 0.6913 - val_loss: 0.8249 - val_accuracy: 0.6152\n"
     ]
    }
   ],
   "source": [
    "history = model_1.fit(X_train, y_train, epochs=10, validation_data=([X_test], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve your sentiment model coupling reviewer embeddings and word embeddigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"classif.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewer classification model \n",
    "Create a second sentiment model that will solely use the reviewer embeddings to predict the restaurant rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T10:58:35.524239Z",
     "start_time": "2020-03-08T10:58:35.520773Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "n2v = Word2Vec.load(\"node2vec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:34:59.689727Z",
     "start_time": "2020-03-08T11:34:59.665068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_documents = df_cap[['review_content', 'rest_rating', 'reviewer_pseudo']].head(DATASET_SIZE).dropna().reset_index(drop=True)\n",
    "labeled_documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a second sentiment model that will solely use the reviewer embeddings to predict the sentiment\n",
    "2. Merge the first and two models to complete the above architecture. \n",
    "\n",
    "\n",
    "You will be using the functional API of keras. \n",
    "\n",
    "#### Proposed architecture \n",
    "**Review NLP model**  \n",
    "1. Input Shape : the input text sequence of max size N (including padding)\n",
    "2. Embedding Layers: mapping the embedding matrix with your input sequence\n",
    "3. Dense layer: linear projection of the previous embedding\n",
    "3. Global average Pooling 1D: \n",
    "4. Flatten: just flattening the representation into a vector, this is the review embedding ! \n",
    "\n",
    "**Reviewer (meta features) model**\n",
    "5. Input Shape : the input reviewer embedding of size N' (dimension you choose for node2vec)\n",
    "\n",
    "\n",
    "**Merging the two models**\n",
    "6. Concatenate layer merging the two models input. \n",
    "7. Global average Pooling 1D: \n",
    "8. Flatten: Final representation before classification\n",
    "9. Dense layer with softmax activation of size corresponding to the number of class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:19.214227Z",
     "start_time": "2020-03-08T11:39:19.151673Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(400, input_shape=(64,)),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "sentiment_model2.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:19.588723Z",
     "start_time": "2020-03-08T11:39:19.503283Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews_padding64 = tf.keras.preprocessing.sequence.pad_sequences(sequences_reviews, maxlen=64)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(reviews_padding64, sentiment, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:26.138452Z",
     "start_time": "2020-03-08T11:39:21.114970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/10\n",
      "6700/6700 [==============================] - 1s 103us/sample - loss: 2242.0408 - accuracy: 0.3893 - val_loss: 808.9950 - val_accuracy: 0.4509\n",
      "Epoch 2/10\n",
      "6700/6700 [==============================] - 0s 57us/sample - loss: 424.5751 - accuracy: 0.4137 - val_loss: 154.0997 - val_accuracy: 0.4221\n",
      "Epoch 3/10\n",
      "6700/6700 [==============================] - 0s 59us/sample - loss: 149.9927 - accuracy: 0.4121 - val_loss: 82.6500 - val_accuracy: 0.3355\n",
      "Epoch 4/10\n",
      "6700/6700 [==============================] - 0s 56us/sample - loss: 91.6679 - accuracy: 0.4082 - val_loss: 112.4686 - val_accuracy: 0.3415\n",
      "Epoch 5/10\n",
      "6700/6700 [==============================] - 0s 56us/sample - loss: 114.2989 - accuracy: 0.4127 - val_loss: 85.8889 - val_accuracy: 0.4691\n",
      "Epoch 6/10\n",
      "6700/6700 [==============================] - 1s 101us/sample - loss: 107.5613 - accuracy: 0.4137 - val_loss: 117.2113 - val_accuracy: 0.4909\n",
      "Epoch 7/10\n",
      "6700/6700 [==============================] - 1s 90us/sample - loss: 108.5712 - accuracy: 0.4191 - val_loss: 85.2462 - val_accuracy: 0.3555\n",
      "Epoch 8/10\n",
      "6700/6700 [==============================] - 0s 70us/sample - loss: 95.9484 - accuracy: 0.4093 - val_loss: 71.7183 - val_accuracy: 0.4091\n",
      "Epoch 9/10\n",
      "6700/6700 [==============================] - 0s 73us/sample - loss: 108.3404 - accuracy: 0.4216 - val_loss: 77.2959 - val_accuracy: 0.5197\n",
      "Epoch 10/10\n",
      "6700/6700 [==============================] - 1s 75us/sample - loss: 117.7497 - accuracy: 0.4109 - val_loss: 113.8180 - val_accuracy: 0.3933\n"
     ]
    }
   ],
   "source": [
    "history = sentiment_model2.fit(\n",
    "    X2_train, y2_train, epochs=10, validation_data=(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:35:42.623384Z",
     "start_time": "2020-03-08T11:35:42.479809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 300)         5567700   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, None, 400)         120400    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 5)                 2005      \n",
      "=================================================================\n",
      "Total params: 5,850,505\n",
      "Trainable params: 5,850,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True),\n",
    "    tf.keras.layers.Dense(400),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last'),\n",
    "    tf.keras.layers.Dense(400, input_shape=(64,)),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "sentiment_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:35:44.651933Z",
     "start_time": "2020-03-08T11:35:44.502662Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 743)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 743, 300)     5567700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 743, 400)     120400      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 400)          0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 400)          0           global_average_pooling1d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 464)          0           flatten[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 400)          186000      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 5)            2005        dense_33[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,876,105\n",
      "Trainable params: 5,876,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "n_classes = 5\n",
    "\n",
    "input_1 = tf.keras.layers.Input(shape=(743,))\n",
    "emb = tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True)(input_1)\n",
    "dense = tf.keras.layers.Dense(400)(emb)\n",
    "pool = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(dense)\n",
    "flatten = tf.keras.layers.Flatten()(pool)\n",
    "tf.keras.layers.Dense(5)\n",
    "\n",
    "input_2 = tf.keras.layers.Input(shape=(64,))\n",
    "\n",
    "\n",
    "# Concatenate\n",
    "concat = tf.keras.layers.Concatenate()([flatten, input_2])\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(400)(concat)\n",
    "\n",
    "# output layer\n",
    "output = tf.keras.layers.Dense(units=n_classes,\n",
    "                               activation=tf.keras.activations.softmax)(dense_2)\n",
    "    \n",
    "full_model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output])\n",
    "\n",
    "full_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "print(full_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:36:10.630628Z",
     "start_time": "2020-03-08T11:36:10.620861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6700, 608), (6700, 64), (6700, 5), (3300, 5))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X2_train.shape, y_train.shape, y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:36:46.477509Z",
     "start_time": "2020-03-08T11:36:46.448963Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (743,) but got array with shape (608,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-c569a279510b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda/envs/py4business/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (743,) but got array with shape (608,)"
     ]
    }
   ],
   "source": [
    "history = full_model.fit([X_train, X2_train], y_train, epochs=10, validation_data=([X_test, X2_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
