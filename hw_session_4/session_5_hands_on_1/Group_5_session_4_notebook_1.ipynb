{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X - HEC Embeddings 2 : Advanced Word Representations - Group 5\n",
    "\n",
    "In this practical session, we will focus on word embeddings through word2vec and a simple classification model for sentiment analysis. Once a word2vec skipgram is trained, we can visualize learned word vectors in a reduced space and use them in our classification model.\n",
    "\n",
    "PS: our contibutions are all indicated by the comment <mark>#### FILL THE BLANK(S) ####</mark> in the corresponding cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:04.818397Z",
     "start_time": "2020-03-08T09:31:55.515479Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:09.989272Z",
     "start_time": "2020-03-08T09:32:04.820866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_date_diner</th>\n",
       "      <th>review_has_answer</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_rating_value</th>\n",
       "      <th>review_rating_service</th>\n",
       "      <th>review_rating_atmosphere</th>\n",
       "      <th>review_rating_food</th>\n",
       "      <th>review_title</th>\n",
       "      <th>...</th>\n",
       "      <th>rest_rating_excellent</th>\n",
       "      <th>rest_rating_very_good</th>\n",
       "      <th>rest_rating_neutral</th>\n",
       "      <th>rest_rating_poor</th>\n",
       "      <th>rest_rating_terrible</th>\n",
       "      <th>rest_url</th>\n",
       "      <th>rest_url_menu</th>\n",
       "      <th>rest_adress</th>\n",
       "      <th>rest_description</th>\n",
       "      <th>grp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g191301-d4453079-r728219948</td>\n",
       "      <td>2019-11-22</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birthday Shots shots shots!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g191301-d4453079-r728632295</td>\n",
       "      <td>2019-11-24</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id review_date review_date_diner  \\\n",
       "0  g191301-d4453079-r728219948  2019-11-22        2019-11-01   \n",
       "1  g191301-d4453079-r728632295  2019-11-24        2019-11-01   \n",
       "\n",
       "   review_has_answer  review_rating  review_rating_value  \\\n",
       "0               True            4.5                  NaN   \n",
       "1               True            4.5                  NaN   \n",
       "\n",
       "   review_rating_service  review_rating_atmosphere  review_rating_food  \\\n",
       "0                    NaN                       NaN                 NaN   \n",
       "1                    NaN                       NaN                 NaN   \n",
       "\n",
       "                  review_title  ... rest_rating_excellent  \\\n",
       "0  Birthday Shots shots shots!  ...                   833   \n",
       "1                     Louis!!!  ...                   833   \n",
       "\n",
       "  rest_rating_very_good rest_rating_neutral rest_rating_poor  \\\n",
       "0                   101                43.0             34.0   \n",
       "1                   101                43.0             34.0   \n",
       "\n",
       "  rest_rating_terrible                                           rest_url  \\\n",
       "0                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "1                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "\n",
       "                                       rest_url_menu         rest_adress  \\\n",
       "0  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "1  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "\n",
       "                                    rest_description  grp  \n",
       "0  Perched on the Thames riverside, this beautifu...  cap  \n",
       "1  Perched on the Thames riverside, this beautifu...  cap  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define data path\n",
    "file_path = os.getcwd() + '/clean_full_graph.csv.gzip'\n",
    "\n",
    "# Read csv file with right parameters\n",
    "df_all = pd.read_csv(file_path, \n",
    "                     compression='gzip', \n",
    "                     low_memory=False, \n",
    "                     parse_dates=['review_date', 'review_date_diner'])\n",
    "\n",
    "df_all.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current exercice we'll work with Capgemini Invent's dataset, so that everyone has the same data. Later on you could try to do your own embedding with your scrapped data and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:10.251052Z",
     "start_time": "2020-03-08T09:32:09.992202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_date_diner</th>\n",
       "      <th>review_has_answer</th>\n",
       "      <th>review_rating</th>\n",
       "      <th>review_rating_value</th>\n",
       "      <th>review_rating_service</th>\n",
       "      <th>review_rating_atmosphere</th>\n",
       "      <th>review_rating_food</th>\n",
       "      <th>review_title</th>\n",
       "      <th>...</th>\n",
       "      <th>rest_rating_excellent</th>\n",
       "      <th>rest_rating_very_good</th>\n",
       "      <th>rest_rating_neutral</th>\n",
       "      <th>rest_rating_poor</th>\n",
       "      <th>rest_rating_terrible</th>\n",
       "      <th>rest_url</th>\n",
       "      <th>rest_url_menu</th>\n",
       "      <th>rest_adress</th>\n",
       "      <th>rest_description</th>\n",
       "      <th>grp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g191301-d4453079-r728219948</td>\n",
       "      <td>2019-11-22</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Birthday Shots shots shots!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g191301-d4453079-r728632295</td>\n",
       "      <td>2019-11-24</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>True</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Louis!!!</td>\n",
       "      <td>...</td>\n",
       "      <td>833</td>\n",
       "      <td>101</td>\n",
       "      <td>43.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>https://www.tripadvisor.com/ShowUserReviews-g1...</td>\n",
       "      <td>http://www.revolution-bars.co.uk/bar/london-ri...</td>\n",
       "      <td>4 Whittaker Avenue</td>\n",
       "      <td>Perched on the Thames riverside, this beautifu...</td>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id review_date review_date_diner  \\\n",
       "0  g191301-d4453079-r728219948  2019-11-22        2019-11-01   \n",
       "1  g191301-d4453079-r728632295  2019-11-24        2019-11-01   \n",
       "\n",
       "   review_has_answer  review_rating  review_rating_value  \\\n",
       "0               True            4.5                  NaN   \n",
       "1               True            4.5                  NaN   \n",
       "\n",
       "   review_rating_service  review_rating_atmosphere  review_rating_food  \\\n",
       "0                    NaN                       NaN                 NaN   \n",
       "1                    NaN                       NaN                 NaN   \n",
       "\n",
       "                  review_title  ... rest_rating_excellent  \\\n",
       "0  Birthday Shots shots shots!  ...                   833   \n",
       "1                     Louis!!!  ...                   833   \n",
       "\n",
       "  rest_rating_very_good rest_rating_neutral rest_rating_poor  \\\n",
       "0                   101                43.0             34.0   \n",
       "1                   101                43.0             34.0   \n",
       "\n",
       "  rest_rating_terrible                                           rest_url  \\\n",
       "0                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "1                 65.0  https://www.tripadvisor.com/ShowUserReviews-g1...   \n",
       "\n",
       "                                       rest_url_menu         rest_adress  \\\n",
       "0  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "1  http://www.revolution-bars.co.uk/bar/london-ri...  4 Whittaker Avenue   \n",
       "\n",
       "                                    rest_description  grp  \n",
       "0  Perched on the Thames riverside, this beautifu...  cap  \n",
       "1  Perched on the Thames riverside, this beautifu...  cap  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cap = df_all[df_all.grp == 'cap'].reset_index(drop=True)\n",
    "df_cap.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:10.448198Z",
     "start_time": "2020-03-08T09:32:10.257463Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cap = df_cap[df_cap['review_content'].str.len() >= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Text Encoding\n",
    "This part concerns tokenization and text encoding with TensorFlow modules :\n",
    "\n",
    "*(i) Build the token vocabulary* <br>\n",
    "*(ii) Build a text encoder relying each word to an index, and thus each text to a sequence of word indices* (```list```) <br>\n",
    "*(iii) Build a TensorFlow dataset for word2vec training*\n",
    "\n",
    "\n",
    "1. **Tokenization** : Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:13.079722Z",
     "start_time": "2020-03-08T09:32:10.450391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36865b325d214521866e1223d0dd70b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18557"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_SIZE = 10000\n",
    "\n",
    "df_cap['review_content'] = df_cap['review_content'].apply(lambda x : literal_eval(x)[0])\n",
    "\n",
    "reviews = df_cap['review_content'][:DATASET_SIZE].values.tolist()\n",
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "vocabulary_set = set()\n",
    "\n",
    "for text in tqdm_notebook(reviews) :\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    vocabulary_set.update(tokens)\n",
    "    \n",
    "vocab_size = len(vocabulary_set)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Token Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:13.129665Z",
     "start_time": "2020-03-08T09:32:13.081980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The atmosphere in here is great, we came for birthday drinks and never left, music is on point too, it’s fun and lively. Lewis was super friendly and helpful serving us and even suggested some great tasting shots! Ask for Lewis when you visit!!\n",
      "\n",
      "\n",
      "[4196, 8087, 1456, 17776, 10929, 13024, 7158, 8003, 7970, 14265, 13119, 12032, 9915, 14816, 10109, 10929, 7417, 1951, 14684, 9413, 11777, 4295, 12032, 17575, 18453, 4537, 2135, 4088, 12032, 9584, 11729, 8742, 12032, 4823, 17672, 5912, 13024, 7188, 3153, 15066, 7970, 18453, 12309, 11251, 14382]\n",
      "\n",
      "\n",
      "The atmosphere in here is great we came for birthday drinks and never left music is on point too it s fun and lively Lewis was super friendly and helpful serving us and even suggested some great tasting shots Ask for Lewis when you visit\n"
     ]
    }
   ],
   "source": [
    "token_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n",
    "\n",
    "print(df_cap['review_content'][0])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "token_encoded_text = token_encoder.encode(df_cap['review_content'][0])\n",
    "print(token_encoded_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "token_decoded_text = token_encoder.decode(token_encoded_text)\n",
    "print(token_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:32:13.154983Z",
     "start_time": "2020-03-08T09:32:13.146903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4196 ----> The\n",
      "8087 ----> atmosphere\n",
      "1456 ----> in\n",
      "17776 ----> here\n",
      "10929 ----> is\n",
      "13024 ----> great\n",
      "7158 ----> we\n",
      "8003 ----> came\n",
      "7970 ----> for\n",
      "14265 ----> birthday\n"
     ]
    }
   ],
   "source": [
    "for tk in token_encoded_text[:10] :\n",
    "    \n",
    "    print('{} ----> {}'.format(tk, token_encoder.decode([tk])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Build Learning Dataset**\n",
    "\n",
    "To learn word2vec vectors, we define center and context words. Thus, we concatenate each document, i.e. sequence of word indices to make the moving context window possible.\n",
    "\n",
    "*Question : build this corpus of concatenated encoded documents, i.e. transform a list of* ```DATASET_SIZE``` *lists of indices in one list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:36:49.131196Z",
     "start_time": "2020-03-08T09:36:49.125113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583157"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #### FILL THE BLANK(S) ####\n",
    "sequences_reviews = [token_encoder.encode(review) for review in reviews] \n",
    "all_tokens = [item for sublist in sequences_reviews for item in sublist]\n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.keras.preprocessing.sequence.make_sampling_table``` and ```tf.keras.preprocessing.sequence.skipgrams``` sequence preprocessing functions are useful to build the negative sampled dataset. The ```skipgrams```function allows to browse the entire document (```all_tokens```) to identify positive and negative pairs of center and context words.\n",
    "\n",
    "*Question : use these functions to get skipgram word pairs with labels (positive or negative). You can use the default* ```sampling_factor```, ```window_size```=4 *and generate one* ```negative_samples```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:37:33.625560Z",
     "start_time": "2020-03-08T09:37:33.605506Z"
    }
   },
   "outputs": [],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "\n",
    "# Sampling table to use in skipgram\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(token_encoder.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:00.810862Z",
     "start_time": "2020-03-08T09:37:33.739690Z"
    }
   },
   "outputs": [],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "\n",
    "# Skipgram: X contains pairs of center word and context word, \n",
    "# and y contains label 0 if the sample is negative and 1 if the sample is positive\n",
    "X, y = tf.keras.preprocessing.sequence.skipgrams(all_tokens, token_encoder.vocab_size, window_size=4, sampling_table=sampling_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:00.843562Z",
     "start_time": "2020-03-08T09:38:00.813893Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3771, 5096], [5992, 4012], [2889, 837], [17859, 4537], [8298, 54]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:00.897116Z",
     "start_time": "2020-03-08T09:38:00.874800Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word2vec training, we won't use a validation dataset this time. In eventual further experimentations, you could include it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:19.696968Z",
     "start_time": "2020-03-08T09:38:00.902412Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X)\n",
    "BATCH_SIZE = 2048\n",
    "\n",
    "# threshold_set = int(np.floor(0.8*len(X)))\n",
    "# \n",
    "# X_train, y_train = tf.convert_to_tensor(X[:threshold_set], dtype=tf.int32), tf.convert_to_tensor(y[:threshold_set], dtype=tf.int32)\n",
    "# center_words_train, context_words_train = X_train[:,0], X_train[:,1]\n",
    "# \n",
    "# word2vec_dataset_train = tf.data.Dataset.from_tensor_slices((center_words_train, context_words_train, y_train)).batch(BATCH_SIZE)\n",
    "# \n",
    "# X_test, y_test = tf.convert_to_tensor(X[threshold_set:], dtype=tf.int32), tf.convert_to_tensor(y[threshold_set:], dtype=tf.int32)\n",
    "# center_words_test, context_words_test = X_test[:,0], X_test[:,1]\n",
    "# \n",
    "# word2vec_dataset_test = tf.data.Dataset.from_tensor_slices((center_words_test, context_words_test, y_test)).batch(BATCH_SIZE)\n",
    "\n",
    "X, y = tf.convert_to_tensor(X, dtype=tf.int32), tf.convert_to_tensor(y, dtype=tf.int32)\n",
    "center_words, context_words = X[:,0], X[:,1]\n",
    "\n",
    "# Batch Dataset useful for word2vec training\n",
    "word2vec_dataset = tf.data.Dataset.from_tensor_slices((center_words, context_words, y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec SkipGram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.234409Z",
     "start_time": "2020-03-08T09:38:19.699300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(2048,)\n",
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "# Batches of data: 2048(center_words, context_words, labels) over which we can iterate\n",
    "center_words_batch, context_words_batch, y_batch = next(iter(word2vec_dataset))\n",
    "print(center_words_batch.shape, context_words_batch.shape, y_batch.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **SkipGram Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.246688Z",
     "start_time": "2020-03-08T09:38:36.237037Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, d_model, token_vocab_size):\n",
    "        \n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = token_vocab_size\n",
    "        \n",
    "        # Embedding Layer: Turns positive integers (indexes) into dense vectors of fixed size (here 300)\n",
    "        # As opposed to sparse representation, in an embedding, words are represented by dense vectors \n",
    "        # where a vector represents the projection of the word into a continuous vector space\n",
    "        \n",
    "        self.input_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model, name='input_embedding')\n",
    "        self.output_embedding = tf.keras.layers.Embedding(self.vocab_size, self.d_model, name='output_embedding')\n",
    "        \n",
    "        \n",
    "    def call(self, center_word, context_word):\n",
    "        \n",
    "        center_vector = self.input_embedding(center_word)\n",
    "        context_vector = self.output_embedding(context_word)\n",
    "        \n",
    "        dot_product = tf.math.reduce_sum(tf.multiply(center_vector, context_vector), axis=1)\n",
    "        \n",
    "        return tf.math.sigmoid(dot_product) # loss : from_logits=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18559"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.337516Z",
     "start_time": "2020-03-08T09:38:36.249059Z"
    }
   },
   "outputs": [],
   "source": [
    "skipgram = SkipGram(300, token_encoder.vocab_size)\n",
    "assert skipgram(center_words_batch, context_words_batch).shape[0] == BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Optimization Objective**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.348907Z",
     "start_time": "2020-03-08T09:38:36.343694Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.368622Z",
     "start_time": "2020-03-08T09:38:36.354963Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "# test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.380269Z",
     "start_time": "2020-03-08T09:38:36.373351Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(center_word, context_word, label):\n",
    "    \n",
    "    with tf.GradientTape() as tape :\n",
    "        \n",
    "        prediction = skipgram(center_word, context_word)\n",
    "        loss = loss_object(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, skipgram.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradient, skipgram.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:38:36.385458Z",
     "start_time": "2020-03-08T09:38:36.382476Z"
    }
   },
   "outputs": [],
   "source": [
    "# def test_step(center_word, context_word, label):\n",
    "#     \n",
    "#     prediction = skipgram(center_word, context_word)\n",
    "#     loss = loss_object(label, prediction)\n",
    "# \n",
    "#     test_loss(loss)\n",
    "#     test_accuracy(label, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch can take almost 10 minutes so define a small number of epochs to end the hands on !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:55:10.834992Z",
     "start_time": "2020-03-08T09:38:36.387695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa950f61988b4587a7967c04a2ea6e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='T R A I N I N G', max=1, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4e943ff6c14445bef67f847f365a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 1/1', max=3729, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.282121866941452 - Accuracy : 89.23088836669922\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### FILL THE BLANK(S) ####\n",
    "EPOCHS = 1\n",
    "\n",
    "DATASET_LENGTH = X.shape[0] # threshold_set\n",
    "\n",
    "\n",
    "for epoch in tqdm_notebook(iterable = range(EPOCHS), total = EPOCHS, desc = 'T R A I N I N G') :\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    # test_loss.reset_states()\n",
    "    # test_accuracy.reset_states()\n",
    "    \n",
    "    for center_word, context_word, label in tqdm_notebook(word2vec_dataset, \n",
    "                                                          total = int(np.ceil(DATASET_LENGTH/BATCH_SIZE)), \n",
    "                                                          desc = 'Epoch {}/{}'.format(epoch+1, EPOCHS)) : \n",
    "        \n",
    "        train_step(center_word, context_word, label)\n",
    "    \n",
    "    \n",
    "    # for center_word, context_word, label in word2vec_dataset_test :\n",
    "    #     \n",
    "    #     test_step(center_word, context_word, label)\n",
    "    \n",
    "\n",
    "    print ('Loss : {} - Accuracy : {}\\n'.format(train_loss.result(), \n",
    "                                                train_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:55:10.840467Z",
     "start_time": "2020-03-08T09:55:10.837772Z"
    }
   },
   "outputs": [],
   "source": [
    "skipgram.save_weights(\"./checkpoints/skipgram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T09:55:10.848195Z",
     "start_time": "2020-03-08T09:55:10.843721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"skip_gram\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_embedding (Embedding)  multiple                  5567700   \n",
      "_________________________________________________________________\n",
      "output_embedding (Embedding) multiple                  5567700   \n",
      "=================================================================\n",
      "Total params: 11,135,400\n",
      "Trainable params: 11,135,400\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Questions : get the weights of the* ```input_embedding``` *layer and store them in a variable* ```weights```. *These weights correspond to word2vec skipgram embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T10:40:32.453541Z",
     "start_time": "2020-03-08T10:40:32.410388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00212635  0.04572015 -0.04756665 ...  0.04842519  0.01440189\n",
      "  -0.01315816]\n",
      " [-0.03470327  0.02726367 -0.01445467 ... -0.03554288  0.02213756\n",
      "   0.03557621]\n",
      " [ 0.02328182 -0.0031823   0.04568103 ... -0.03926669 -0.01174574\n",
      "   0.0374235 ]\n",
      " ...\n",
      " [-0.03097587 -0.03813989 -0.03270343 ...  0.03742202 -0.00967305\n",
      "  -0.00831021]\n",
      " [-0.1178622  -0.0528922  -0.10733299 ... -0.10590317 -0.13432115\n",
      "   0.12354109]\n",
      " [ 0.0474226  -0.04316493  0.04249083 ... -0.00072579 -0.03979396\n",
      "   0.04987754]]\n",
      "(18559, 300)\n"
     ]
    }
   ],
   "source": [
    "#### FILL THE BLANK(S) ####\n",
    "\n",
    "# These are the embeddings for each word in the vocabulary, \n",
    "# a sparse representation of each word in a space of dim 300\n",
    "\n",
    "weights = (skipgram.get_weights()[0])\n",
    "\n",
    "print(weights, weights.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question : use the following cell to store the learned vectors in the correct format and visualize them in the Embedding Projector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T10:42:59.082130Z",
     "start_time": "2020-03-08T10:42:55.140959Z"
    }
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for idx, word in enumerate(token_encoder.tokens):\n",
    "    \n",
    "    vec = weights[idx+1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize your trained embeddings, open the [Embedding Projector](http://projector.tensorflow.org/) :\n",
    "\n",
    "- Click on \"Load data\".\n",
    "- Upload the two files we created above : vecs.tsv and meta.tsv.\n",
    "\n",
    "The embeddings will now be displayed. You can search for words from ```token_encoder.tokens``` to find their closest neighbors.\n",
    "\n",
    "# Text Classication : Sentiment Analysis\n",
    "Our learned word embeddings can be used to represent the words of a text and to build a text representation. This text representation will be useful for classifcation.\n",
    "\n",
    "*Question : in this part, our objective is to train a text classification model for sentiment analysis using the first* ```DATASET_SIZE``` reviews*. You have to :*\n",
    "1. *Build the learning dataset : use the ```review_content``` column for text inputs and ```rest_rating``` for sentiments (outputs).* **NB** *: Don't forget to encode and pad your text, and use one-hot encoding for sentiment labels.*\n",
    "2. *Once the learning dataset built, you have to split it into train and validation datasets.*\n",
    "3. *Define and train a text classification model.*\n",
    "\n",
    "As indications, you can use ```tf.keras.preprocessing.sequence.pad_sequences``` for text padding and ```tf.keras.utils.to_categorical``` for one-hot label encoding. For your first model, you can already use the simple text classification model described in the course with ```tf.keras.Sequential``` API :\n",
    "1. An embedding layer : if you use zero padding, you can set ```mask_zero=True``` and use the ```weights``` matrix for initialization.\n",
    "2. A dense layer without particular activation function for linear projection of the previous embedding vectors.\n",
    "3. A global average pooling (1D).\n",
    "4. A final dense layer for linear projection in a $d$-dimensional space for sentiment prediction, with $d$ the number of possible sentiments/classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:30.628000Z",
     "start_time": "2020-03-08T11:39:30.618107Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.5, 5. , 4. , 3.5, 3. ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cap['rest_rating'][:DATASET_SIZE].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:31.101331Z",
     "start_time": "2020-03-08T11:39:30.972295Z"
    }
   },
   "outputs": [],
   "source": [
    "##### FILL THE BLANKS\n",
    "\n",
    "# PS: to_categorial starts from 0 so it creates an extra class if there is no 0 \n",
    "# thus the \"-3\" so that the ratings start from 0 \n",
    "rest_rating_from0 = df_cap['rest_rating'][:DATASET_SIZE]-3\n",
    "sentiment = tf.keras.utils.to_categorical(rest_rating_from0, num_classes=5)\n",
    "# Padding (making all encoded reviews of the length of the longest)\n",
    "content_seq = tf.keras.preprocessing.sequence.pad_sequences(sequences_reviews)\n",
    "X_train, X_test, y_train, y_test = train_test_split(content_seq, sentiment, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:31.598454Z",
     "start_time": "2020-03-08T11:39:31.593099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 608), (10000, 5))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_seq.shape, sentiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:32.296600Z",
     "start_time": "2020-03-08T11:39:32.156224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 300)         5567700   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 400)         120400    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2005      \n",
      "=================================================================\n",
      "Total params: 5,690,105\n",
      "Trainable params: 5,690,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##### FILL THE BLANKS\n",
    "\n",
    "# Text classification model for sentiment analysis\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True),\n",
    "    tf.keras.layers.Dense(400),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last'),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:45:58.882260Z",
     "start_time": "2020-03-08T11:39:33.061695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6700 samples, validate on 3300 samples\n",
      "Epoch 1/10\n",
      "6700/6700 [==============================] - 60s 9ms/sample - loss: 1.0484 - accuracy: 0.5412 - val_loss: 0.9645 - val_accuracy: 0.5573\n",
      "Epoch 2/10\n",
      "6700/6700 [==============================] - 55s 8ms/sample - loss: 0.9695 - accuracy: 0.5466 - val_loss: 0.9529 - val_accuracy: 0.5573\n",
      "Epoch 3/10\n",
      "6700/6700 [==============================] - 58s 9ms/sample - loss: 0.9569 - accuracy: 0.5467 - val_loss: 0.9431 - val_accuracy: 0.5573\n",
      "Epoch 4/10\n",
      "6700/6700 [==============================] - 49s 7ms/sample - loss: 0.9416 - accuracy: 0.5473 - val_loss: 0.9335 - val_accuracy: 0.5573\n",
      "Epoch 5/10\n",
      "6700/6700 [==============================] - 46s 7ms/sample - loss: 0.9200 - accuracy: 0.5509 - val_loss: 0.9291 - val_accuracy: 0.5697\n",
      "Epoch 6/10\n",
      "6700/6700 [==============================] - 51s 8ms/sample - loss: 0.8924 - accuracy: 0.5627 - val_loss: 0.8998 - val_accuracy: 0.5627\n",
      "Epoch 7/10\n",
      "6700/6700 [==============================] - 49s 7ms/sample - loss: 0.8560 - accuracy: 0.5851 - val_loss: 0.8784 - val_accuracy: 0.5858\n",
      "Epoch 8/10\n",
      "6700/6700 [==============================] - 59s 9ms/sample - loss: 0.8141 - accuracy: 0.6254 - val_loss: 0.8625 - val_accuracy: 0.5994\n",
      "Epoch 9/10\n",
      "6700/6700 [==============================] - 54s 8ms/sample - loss: 0.7653 - accuracy: 0.6645 - val_loss: 0.8443 - val_accuracy: 0.6194\n",
      "Epoch 10/10\n",
      "6700/6700 [==============================] - 60s 9ms/sample - loss: 0.7223 - accuracy: 0.6904 - val_loss: 0.8265 - val_accuracy: 0.6191\n"
     ]
    }
   ],
   "source": [
    "history = model_1.fit(X_train, y_train, epochs=10, validation_data=([X_test], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our first model achieves a validation accuracy of 0.6191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve your sentiment model coupling reviewer embeddings and word embeddigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"classif.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviewer classification model \n",
    "Create a second sentiment model that will solely use the reviewer embeddings to predict the restaurant rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T10:58:35.524239Z",
     "start_time": "2020-03-08T10:58:35.520773Z"
    }
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Word2Vec\n",
    "#n2v = Word2Vec.load(\"node2vec.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:34:59.689727Z",
     "start_time": "2020-03-08T11:34:59.665068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_documents = df_cap[['review_content', 'rest_rating', 'reviewer_pseudo']].head(DATASET_SIZE).dropna().reset_index(drop=True)\n",
    "labeled_documents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a second sentiment model that will solely use the reviewer embeddings to predict the sentiment\n",
    "2. Merge the first and two models to complete the above architecture. \n",
    "\n",
    "\n",
    "You will be using the functional API of keras. \n",
    "\n",
    "#### Proposed architecture \n",
    "**Review NLP model**  \n",
    "1. Input Shape : the input text sequence of max size N (including padding)\n",
    "2. Embedding Layers: mapping the embedding matrix with your input sequence\n",
    "3. Dense layer: linear projection of the previous embedding\n",
    "3. Global average Pooling 1D: \n",
    "4. Flatten: just flattening the representation into a vector, this is the review embedding ! \n",
    "\n",
    "**Reviewer (meta features) model**\n",
    "5. Input Shape : the input reviewer embedding of size N' (dimension you choose for node2vec)\n",
    "\n",
    "\n",
    "**Merging the two models**\n",
    "6. Concatenate layer merging the two models input. \n",
    "7. Global average Pooling 1D: \n",
    "8. Flatten: Final representation before classification\n",
    "9. Dense layer with softmax activation of size corresponding to the number of class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:19.214227Z",
     "start_time": "2020-03-08T11:39:19.151673Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(400, input_shape=(64,)),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "sentiment_model2.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:19.588723Z",
     "start_time": "2020-03-08T11:39:19.503283Z"
    }
   },
   "outputs": [],
   "source": [
    "reviews_padding64 = tf.keras.preprocessing.sequence.pad_sequences(sequences_reviews, maxlen=64)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(reviews_padding64, sentiment, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:39:26.138452Z",
     "start_time": "2020-03-08T11:39:21.114970Z"
    }
   },
   "outputs": [],
   "source": [
    "history = sentiment_model2.fit(\n",
    "    X2_train, y2_train, epochs=10, validation_data=(X2_test, y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:35:42.623384Z",
     "start_time": "2020-03-08T11:35:42.479809Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True),\n",
    "    tf.keras.layers.Dense(400),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last'),\n",
    "    tf.keras.layers.Dense(400, input_shape=(64,)),\n",
    "    tf.keras.layers.Dense(5)\n",
    "])\n",
    "\n",
    "sentiment_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "sentiment_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:35:44.651933Z",
     "start_time": "2020-03-08T11:35:44.502662Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "n_classes = 5\n",
    "\n",
    "input_1 = tf.keras.layers.Input(shape=(743,))\n",
    "emb = tf.keras.layers.Embedding(token_encoder.vocab_size, 300, mask_zero=True, weights=[weights], trainable=True)(input_1)\n",
    "dense = tf.keras.layers.Dense(400)(emb)\n",
    "pool = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_last')(dense)\n",
    "flatten = tf.keras.layers.Flatten()(pool)\n",
    "tf.keras.layers.Dense(5)\n",
    "\n",
    "input_2 = tf.keras.layers.Input(shape=(64,))\n",
    "\n",
    "\n",
    "# Concatenate\n",
    "concat = tf.keras.layers.Concatenate()([flatten, input_2])\n",
    "\n",
    "dense_2 = tf.keras.layers.Dense(400)(concat)\n",
    "\n",
    "# output layer\n",
    "output = tf.keras.layers.Dense(units=n_classes,\n",
    "                               activation=tf.keras.activations.softmax)(dense_2)\n",
    "    \n",
    "full_model = tf.keras.Model(inputs=[input_1, input_2], outputs=[output])\n",
    "\n",
    "full_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                        metrics=['accuracy'])\n",
    "print(full_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:36:10.630628Z",
     "start_time": "2020-03-08T11:36:10.620861Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, X2_train.shape, y_train.shape, y2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T11:50:52.542822Z",
     "start_time": "2020-03-08T11:50:52.539486Z"
    }
   },
   "outputs": [],
   "source": [
    "history = full_model.fit([X_train, X2_train], y_train, epochs=10, validation_data=([X_test, X2_test], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
