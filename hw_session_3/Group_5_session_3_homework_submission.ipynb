{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Word Embedding & Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal and outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our goal is to build a simple classification model and evaluate its performance when using the embedding matrices produced by three different embedding techniques: LSI, Word2Vec and FastText. \n",
    "\n",
    "In the three following sections we build the embedding matrices using each technique. Then, in section 4, we try to fit a cosine similarity classifier and a random forest classifier and evaluate its performances in predicting the ratings of the reviews as a target variable, with respect to the embeding matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os.path\n",
    "\n",
    "# Text manipulation\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP Modules\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, similarities\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Vizualisation\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "\n",
    "# Extra imports\n",
    "\n",
    "# Uncomment the following lines if you haven't installed gensim and nltk\n",
    "#!pip3 install gensim\n",
    "#!pip3 install nltk\n",
    "\n",
    "# Downloading useful nltk packages if not already done\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[decid, visit, windsor, castl, way, back, sw, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[good, although, rather, small, portion, howev...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[look, somewher, budget, go, eat, overnight, w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[good, menu, select, unfortun, stifado, avail,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pop, last, night, glass, wine, attend, theatr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     stemmed_reviews  rating\n",
       "0  [decid, visit, windsor, castl, way, back, sw, ...       5\n",
       "1  [good, although, rather, small, portion, howev...       2\n",
       "2  [look, somewher, budget, go, eat, overnight, w...       5\n",
       "3  [good, menu, select, unfortun, stifado, avail,...       4\n",
       "4  [pop, last, night, glass, wine, attend, theatr...       3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we use functions we defined in previous homework, running this cell takes few minutes\n",
    "\n",
    "def load_data(DATA_PATH = \"data/\", file_name = \"raw_scrapped_data.csv.gzip\"):\n",
    "    \"\"\"\n",
    "    Input  : path of where data is stored\n",
    "    Purpose: loading csv file of reviews\n",
    "    Output : data frame of reviews with associated ratings\n",
    "    \"\"\"    \n",
    "    # Path of the file\n",
    "    file_path = DATA_PATH + file_name\n",
    "\n",
    "    # Reading data\n",
    "    scrapped_data = pd.read_csv(file_path, compression='gzip')\n",
    "    data = scrapped_data[['content', 'rating']]\n",
    "    return data \n",
    "\n",
    "def basic_cleaning(series):\n",
    "    # Remove punctuation\n",
    "    new_series = series.str.replace('[^\\w\\s]','')\n",
    "    # Strip trailing whitespace\n",
    "    new_series = new_series.str.strip(\" \")\n",
    "    # Decapitalize letters\n",
    "    new_series = new_series.apply(lambda x: str(x).lower())\n",
    "    return new_series\n",
    "\n",
    "def tokenize_filter(sentence):\n",
    "    # Define stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    ## Add personalised stop words\n",
    "    stop_words |= set([\"london\", \"food\", \"drink\", \"restaurant\"])\n",
    "    # Filter the sentence\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return (word_tokens, filtered_sentence)\n",
    "\n",
    "def stem_review(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    return tokens.apply(lambda x: [porter.stem(x[i]) for i in range(len(x))])\n",
    "\n",
    "def preprocess_data(data):\n",
    "    df = data\n",
    "    df[\"clean_content\"] = basic_cleaning(df[\"content\"])\n",
    "    df[\"tokenized_content\"] = df[\"clean_content\"].apply(lambda x: tokenize_filter(x)[1])\n",
    "    df[\"stemmed_reviews\"] = stem_review(df[\"tokenized_content\"])\n",
    "    return df[['stemmed_reviews', 'rating']]\n",
    "\n",
    "df = preprocess_data(load_data())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latent semantic indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the functions already defined in the handout and adapt them to our particular case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 100 reviews for now\n",
    "reduced_df = df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using \"corpora\" from gensim to extract vocabulary from a corpus\n",
    "def get_dictionary(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the whole associated vocabulary\n",
    "    Output : term dictionary\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our corpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    return corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Buildind TF matrix useful for LSI\n",
    "def get_TF_matrix(doc_clean, useTransfertDict=True):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the term frequency matrix from a corpus\n",
    "    Output : Document Term Frequency Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "        \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    return [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== Training LSI model report ====================\n",
      "\n",
      "Initial TF matrix (NwordsXNdocuments): \n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [2 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]]\n",
      "\n",
      "Derivation of Term Matrix T of Training Document Word Stems: \n",
      "[[ 2.35550898e-03  3.51214906e-04  4.12257024e-03 ...  2.30976527e-03\n",
      "   1.15488263e-03  1.15488263e-03]\n",
      " [ 5.21497239e-03 -7.20731810e-05  6.71661400e-03 ... -5.63977793e-03\n",
      "  -2.81988896e-03 -2.81988896e-03]\n",
      " [ 5.49123910e-03  5.71855628e-04  5.52210733e-03 ... -2.69197419e-03\n",
      "  -1.34598709e-03 -1.34598709e-03]\n",
      " ...\n",
      " [ 7.47644070e-03  5.50465870e-03  2.19724503e-02 ... -1.72647380e-03\n",
      "  -8.63236901e-04 -8.63236901e-04]\n",
      " [-6.49218673e-03 -6.34558874e-03 -3.41969377e-03 ...  9.99042438e-04\n",
      "   4.99521219e-04  4.99521219e-04]\n",
      " [ 4.79715088e-03  2.03058037e-03  6.25104745e-03 ...  1.19538433e-03\n",
      "   5.97692163e-04  5.97692163e-04]]\n",
      "\n",
      "LSI Vectors of Training Document Word Stems: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Create an LSI model using Gensim\n",
    "def create_gensim_lsi_model(clean_documents_list, k=None):\n",
    "    \"\"\"\n",
    "    Input  : clean document, dictionary\n",
    "    Purpose: create LSI model (Latent Semantic Indexing) \n",
    "             from corpus and dictionary\n",
    "    Output : return LSI model\n",
    "    \"\"\"\n",
    "    \n",
    "    #LSI model consists of Singular Value Decomposition (SVD) of\n",
    "    #Term Document Matrix M: M = T x S x D'\n",
    "    #and dimensionality reductions of T, S and D (\"Derivation\")\n",
    "    \n",
    "    dictionary = get_dictionary(clean_documents_list)\n",
    "    \n",
    "    corpus = get_TF_matrix(clean_documents_list)\n",
    "    if k is not None:\n",
    "        lsi_model = LsiModel(\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=int(k)\n",
    "                )\n",
    "    else:\n",
    "            lsi_model = LsiModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary \n",
    "            )\n",
    "    print(); print(); print(\"=\"*20, \"Training LSI model report\", \"=\"*20); print()\n",
    "    \n",
    "    print(\"Initial TF matrix (NwordsXNdocuments): \")\n",
    "    TF = []\n",
    "    for x in corpus:\n",
    "        wrds = [0 for i in range(len(dictionary))]\n",
    "        for i, j in x: wrds[i] = j\n",
    "        TF.append(wrds)\n",
    "    print(pd.np.transpose(TF))\n",
    "    print()\n",
    "    print(\"Derivation of Term Matrix T of Training Document Word Stems: \")\n",
    "    print(lsi_model.get_topics())\n",
    "    print()\n",
    "    #Derivation of Term Document Matrix of Training Document Word Stems = M' x [Derivation of T]\n",
    "    print(\"LSI Vectors of Training Document Word Stems: \")\n",
    "    print([lsi_model[document_word_stems] for document_word_stems in corpus])\n",
    "    print(\"=\"*70); print(); print()\n",
    "    return lsi_model\n",
    "\n",
    "def get_lsi_vector(lsi_model, clean_text):\n",
    "    return lsi_model[dictionary.doc2bow(clean_text)]\n",
    "\n",
    "# create lsi model\n",
    "lsi_model = create_gensim_lsi_model(reduced_df.stemmed_reviews)\n",
    "# build encoded corpus (TF matrix)\n",
    "corpus_TFmatrix = get_TF_matrix(reduced_df.stemmed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiating the model from gensim models\n",
    "def create_word2vec_model():\n",
    "    \"\"\"\n",
    "    Input  : none\n",
    "    Purpose: create word2vec model from corpus\n",
    "    Output : term dictionary\n",
    "    \"\"\"\n",
    "    path = get_tmpfile(\"word2vec.model\")\n",
    "    model = gensim.models.Word2Vec(size=300, \n",
    "                                   window=3, \n",
    "                                   min_count=5, \n",
    "                                   workers=4, \n",
    "                                   seed=1, \n",
    "                                   iter=50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Building model vocabulary\n",
    "def init_vocab(model, clean_documents_list):\n",
    "    \"\"\"\n",
    "    Input  : model and clean documents list\n",
    "    Purpose: instantiate model vocabulary from clean documents list\n",
    "    Output : model with vocabulary\n",
    "    \"\"\"\n",
    "    init_vocab = list(map(lambda review: review, clean_documents_list[\"stemmed_reviews\"]))\n",
    "    model.build_vocab(init_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training the model on the reviews, then saving it\n",
    "def train_word2vec_model(model, clean_documents_list):\n",
    "    \"\"\"\n",
    "    Input  : model and clean documents list\n",
    "    Purpose: train model on clean documents list\n",
    "    Output : trained model\n",
    "    \"\"\"\n",
    "    corpus = list(map(lambda review: review, clean_documents_list[\"stemmed_reviews\"]))\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save(\"word2vec.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loading pre-trained model\n",
    "def load_trained_model(model_name):\n",
    "    \"\"\"\n",
    "    Input  : trained model name\n",
    "    Purpose: load trained model\n",
    "    Output : saved model\n",
    "    \"\"\"\n",
    "    return gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Getting embedded matrix from corpus\n",
    "def get_word2vec_matrix(model):\n",
    "    embedding_matrix = dict()\n",
    "    for word in model.wv.vocab.keys():\n",
    "        embedding_matrix[word] = list(model.wv[word])\n",
    "    return pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEON\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\LEON\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('superb', 0.792310357093811),\n",
       " ('excel', 0.7652319669723511),\n",
       " ('brilliant', 0.7343354225158691),\n",
       " ('outstand', 0.7075722217559814),\n",
       " ('love', 0.6906702518463135),\n",
       " ('spot', 0.6830048561096191),\n",
       " ('faultless', 0.6820341348648071),\n",
       " ('fantast', 0.6801029443740845),\n",
       " ('fun', 0.67723548412323),\n",
       " ('relax', 0.6718858480453491)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating and training model\n",
    "model = create_word2vec_model()\n",
    "model = init_vocab(model, reduced_df)\n",
    "model = train_word2vec_model(model, reduced_df)\n",
    "model = load_trained_model(\"word2vec.model\")\n",
    "\n",
    "# Testing for an example\n",
    "model.most_similar(\"great\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1278)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decid</th>\n",
       "      <th>visit</th>\n",
       "      <th>way</th>\n",
       "      <th>back</th>\n",
       "      <th>england</th>\n",
       "      <th>saw</th>\n",
       "      <th>establish</th>\n",
       "      <th>english</th>\n",
       "      <th>beer</th>\n",
       "      <th>thought</th>\n",
       "      <th>...</th>\n",
       "      <th>appetit</th>\n",
       "      <th>broken</th>\n",
       "      <th>pipe</th>\n",
       "      <th>45</th>\n",
       "      <th>court</th>\n",
       "      <th>dairi</th>\n",
       "      <th>sorbet</th>\n",
       "      <th>mother</th>\n",
       "      <th>effort</th>\n",
       "      <th>aunt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.412417</td>\n",
       "      <td>0.476795</td>\n",
       "      <td>-0.106598</td>\n",
       "      <td>0.162331</td>\n",
       "      <td>0.107145</td>\n",
       "      <td>-0.215314</td>\n",
       "      <td>-0.017548</td>\n",
       "      <td>0.241625</td>\n",
       "      <td>0.038465</td>\n",
       "      <td>-0.130219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250671</td>\n",
       "      <td>-0.366363</td>\n",
       "      <td>-0.224248</td>\n",
       "      <td>-0.296203</td>\n",
       "      <td>-0.204752</td>\n",
       "      <td>-0.293993</td>\n",
       "      <td>-0.292644</td>\n",
       "      <td>-0.147118</td>\n",
       "      <td>-0.092475</td>\n",
       "      <td>-0.296086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.003393</td>\n",
       "      <td>-0.165455</td>\n",
       "      <td>-0.138401</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>-0.067063</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>-0.075753</td>\n",
       "      <td>-0.124977</td>\n",
       "      <td>-0.082378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007569</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.053698</td>\n",
       "      <td>0.041371</td>\n",
       "      <td>-0.014640</td>\n",
       "      <td>-0.062441</td>\n",
       "      <td>0.058857</td>\n",
       "      <td>-0.025711</td>\n",
       "      <td>-0.029227</td>\n",
       "      <td>0.013302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.226729</td>\n",
       "      <td>-0.494166</td>\n",
       "      <td>-0.219555</td>\n",
       "      <td>0.262298</td>\n",
       "      <td>-0.088872</td>\n",
       "      <td>-0.110763</td>\n",
       "      <td>-0.103431</td>\n",
       "      <td>-0.245499</td>\n",
       "      <td>0.192870</td>\n",
       "      <td>-0.001141</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144690</td>\n",
       "      <td>-0.058607</td>\n",
       "      <td>-0.250377</td>\n",
       "      <td>-0.062961</td>\n",
       "      <td>-0.142785</td>\n",
       "      <td>-0.006058</td>\n",
       "      <td>-0.105786</td>\n",
       "      <td>-0.173935</td>\n",
       "      <td>-0.229302</td>\n",
       "      <td>-0.047576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238297</td>\n",
       "      <td>0.416677</td>\n",
       "      <td>-0.029450</td>\n",
       "      <td>0.095993</td>\n",
       "      <td>0.155644</td>\n",
       "      <td>0.179707</td>\n",
       "      <td>0.077634</td>\n",
       "      <td>0.178074</td>\n",
       "      <td>-0.086265</td>\n",
       "      <td>-0.028404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029356</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.119931</td>\n",
       "      <td>0.029547</td>\n",
       "      <td>0.127596</td>\n",
       "      <td>-0.098690</td>\n",
       "      <td>-0.030303</td>\n",
       "      <td>0.155667</td>\n",
       "      <td>0.108431</td>\n",
       "      <td>-0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.068234</td>\n",
       "      <td>0.122572</td>\n",
       "      <td>0.043795</td>\n",
       "      <td>0.266209</td>\n",
       "      <td>0.114518</td>\n",
       "      <td>0.122392</td>\n",
       "      <td>0.242520</td>\n",
       "      <td>0.123978</td>\n",
       "      <td>-0.222509</td>\n",
       "      <td>0.049515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>-0.010640</td>\n",
       "      <td>0.017679</td>\n",
       "      <td>0.041814</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>0.030447</td>\n",
       "      <td>-0.026125</td>\n",
       "      <td>-0.063303</td>\n",
       "      <td>0.085140</td>\n",
       "      <td>0.032656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      decid     visit       way      back   england       saw  establish  \\\n",
       "0  0.412417  0.476795 -0.106598  0.162331  0.107145 -0.215314  -0.017548   \n",
       "1 -0.003393 -0.165455 -0.138401  0.054900  0.028504 -0.067063   0.056478   \n",
       "2  0.226729 -0.494166 -0.219555  0.262298 -0.088872 -0.110763  -0.103431   \n",
       "3  0.238297  0.416677 -0.029450  0.095993  0.155644  0.179707   0.077634   \n",
       "4 -0.068234  0.122572  0.043795  0.266209  0.114518  0.122392   0.242520   \n",
       "\n",
       "    english      beer   thought  ...   appetit    broken      pipe        45  \\\n",
       "0  0.241625  0.038465 -0.130219  ... -0.250671 -0.366363 -0.224248 -0.296203   \n",
       "1 -0.075753 -0.124977 -0.082378  ... -0.007569  0.004242  0.053698  0.041371   \n",
       "2 -0.245499  0.192870 -0.001141  ... -0.144690 -0.058607 -0.250377 -0.062961   \n",
       "3  0.178074 -0.086265 -0.028404  ... -0.029356 -0.002065  0.119931  0.029547   \n",
       "4  0.123978 -0.222509  0.049515  ...  0.067074 -0.010640  0.017679  0.041814   \n",
       "\n",
       "      court     dairi    sorbet    mother    effort      aunt  \n",
       "0 -0.204752 -0.293993 -0.292644 -0.147118 -0.092475 -0.296086  \n",
       "1 -0.014640 -0.062441  0.058857 -0.025711 -0.029227  0.013302  \n",
       "2 -0.142785 -0.006058 -0.105786 -0.173935 -0.229302 -0.047576  \n",
       "3  0.127596 -0.098690 -0.030303  0.155667  0.108431 -0.002139  \n",
       "4 -0.003292  0.030447 -0.026125 -0.063303  0.085140  0.032656  \n",
       "\n",
       "[5 rows x 1278 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving embedding matrix\n",
    "corpus_word2vecMatrix = get_word2vec_matrix(model)\n",
    "print(corpus_word2vecMatrix.shape)\n",
    "corpus_word2vecMatrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of the performance of a Random Forest on different types of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train/test:  (66, 2) VS (34, 2)\n"
     ]
    }
   ],
   "source": [
    "# First we split train/test data\n",
    "\n",
    "df_dataset = reduced_df\n",
    "n = len(df_dataset)\n",
    "df_dataset.sample(n=n, random_state=16)\n",
    "n = int(2 * n / 3)\n",
    "df_dataset_train = df_dataset[:n]\n",
    "df_dataset_test = df_dataset[n:]\n",
    "print(\"Split train/test: \", df_dataset_train.shape, \"VS\", df_dataset_test.shape)\n",
    "corpus_TFmatrix_train = get_TF_matrix(df_dataset_train.stemmed_reviews)\n",
    "corpus_TFmatrix_test = get_TF_matrix(df_dataset_test.stemmed_reviews)\n",
    "y_train=df_dataset_train.rating\n",
    "y_test=df_dataset_test.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x1eb2b189518>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we use the classifier defined in the handout on our dataset\n",
    "\n",
    "def distance_classifier_cosine_traning(lsi_vector_trainDB):\n",
    "    \"\"\"\n",
    "    Input  : LSI vectors\n",
    "    Purpose: calculate cosine similarity matrix\n",
    "    Output : return similarity matrix\n",
    "    \"\"\"\n",
    "    #calculate cosine similarity matrix for all training document LSI vectors\n",
    "    return similarities.MatrixSimilarity(lsi_vector_trainDB)\n",
    "\n",
    "def distance_classifier_cosine_test(classification_model, training_data, test_doc_lsi_vector, N=1):\n",
    "    \"\"\"\n",
    "    Input  : trained classifier model, the training data (list of descriptions), lsi vectors of a document and N nearest document in the training data base\n",
    "    Purpose: calculate cosine similarity matrix against all training samples\n",
    "    Output : return nearest N document and classes\n",
    "    \"\"\"\n",
    "    cosine_similarities = classification_model[test_doc_lsi_vector]\n",
    "\n",
    "    most_similar_document_test = training_data[np.argmax(cosine_similarities)]\n",
    "\n",
    "    #calculate cosine similarity matrix for all training document LSI vectors\n",
    "    return most_similar_document_test\n",
    "\n",
    "def reco_rate(ref_labels, predicted_labels):\n",
    "    commun_labels = (pd.np.array(ref_labels)==pd.np.array(predicted_labels)).sum()\n",
    "    return 100 * commun_labels / len(ref_labels)\n",
    "\n",
    "classification_model = distance_classifier_cosine_traning(lsi_model[corpus_TFmatrix_train])\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "# We test on train data\n",
    "dictionary = get_dictionary(df_dataset.stemmed_reviews)\n",
    "predicted_ratings = [distance_classifier_cosine_test(classification_model, \n",
    "                                df_dataset_train.rating, \n",
    "                                get_lsi_vector(lsi_model, df_dataset_train.stemmed_reviews.iloc[i]))\n",
    "                                for i in range(df_dataset_train.shape[0])]\n",
    "\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprinsingly, the cosine distance classifier performs perfectly on test data because it by definition the distance of each stemmed review to itself is zero and the decision is easily made. Let's see how it performs on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on test DB: 41.18 %\n"
     ]
    }
   ],
   "source": [
    "# We test on test Data\n",
    "predicted_ratings_test = [distance_classifier_cosine_test(classification_model, \n",
    "                                 df_dataset_train.rating, \n",
    "                                 get_lsi_vector(lsi_model, \n",
    "                                               df_dataset_test.stemmed_reviews.iloc[i]\n",
    "                                              ))\n",
    "                   for i in range(df_dataset_test.shape[0])]\n",
    "\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>[select, lunch, south, african, busi, guest, c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>[miss, love, indonesian, cuisin, holiday, hop,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>[great, good, wine, list, attent, courteou, se...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>[friend, went, weekend, absolut, love, believ,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[first, time, visit, sceneri, took, breath, aw...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[love, pint, beer, littl, els, call, windsor, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[spanish, tapa, best, great, locat, realli, go...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[time, lunch, work, peopl, last, time, worst, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>[stay, close, good, littl, place, good, locat,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[got, introduc, friend, hook, triangl, love, h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      stemmed_reviews  rating\n",
       "66  [select, lunch, south, african, busi, guest, c...       4\n",
       "67  [miss, love, indonesian, cuisin, holiday, hop,...       4\n",
       "68  [great, good, wine, list, attent, courteou, se...       5\n",
       "69  [friend, went, weekend, absolut, love, believ,...       4\n",
       "70  [first, time, visit, sceneri, took, breath, aw...       4\n",
       "71  [love, pint, beer, littl, els, call, windsor, ...       3\n",
       "72  [spanish, tapa, best, great, locat, realli, go...       5\n",
       "73  [time, lunch, work, peopl, last, time, worst, ...       3\n",
       "74  [stay, close, good, littl, place, good, locat,...       4\n",
       "75  [got, introduc, friend, hook, triangl, love, h...       5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take a look at the test reviews and their ratings \n",
    "df_dataset_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 3, 5, 3, 5, 2, 5, 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is our prediction\n",
    "predicted_ratings_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the cosine classifier performs very poorly on test data using LSI embedding, we switch to random forest classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(clf,X_train,y_train):\n",
    "    \"\"\"\n",
    "    Input  : LSI vectors\n",
    "    Purpose: train classification model\n",
    "    Output : return trained classifier\n",
    "    \"\"\"\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf\n",
    "\n",
    "def model_predictions(clf, X_test):\n",
    "    \"\"\"\n",
    "    Input  : trained classifier model, the test set\n",
    "    Purpose: make predictions on test data\n",
    "    Output : predictions\n",
    "    \"\"\"\n",
    "    y_pred=clf.predict(X_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def reco_rate(ref_labels, predicted_labels):\n",
    "    commun_labels = (pd.np.array(ref_labels)==pd.np.array(predicted_labels)).sum()\n",
    "    return 100 * commun_labels / len(ref_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 98.48 %\n",
      "Classifier performances on test DB: 44.12 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEON\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf=model_training(clf,np.array(lsi_model[corpus_TFmatrix_train])[:,:,1],y_train)\n",
    "\n",
    "# We test on train data\n",
    "predicted_ratings = model_predictions(clf, np.array(lsi_model[corpus_TFmatrix_train])[:,:,1])\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")\n",
    "\n",
    "# We test on test Data\n",
    "predicted_ratings_test = model_predictions(clf, np.array(lsi_model[corpus_TFmatrix_test])[:,:,1])\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    ##remove words that aren't in vocabulary\n",
    "    doc = [word for word in doc if word in model.wv.vocab.keys()]\n",
    "    return np.sum(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEON\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for doc in df_dataset_train.stemmed_reviews: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "X_train = np.array(x)\n",
    "\n",
    "x = []\n",
    "for doc in df_dataset_test.stemmed_reviews: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "X_test = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 96.97 %\n",
      "Classifier performances on test DB: 38.24 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEON\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier()\n",
    "clf=model_training(clf,X_train,y_train)\n",
    "\n",
    "# We test on train data\n",
    "predicted_ratings = model_predictions(clf, X_train)\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")\n",
    "\n",
    "# We test on test Data\n",
    "predicted_ratings_test = model_predictions(clf, X_test)\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
