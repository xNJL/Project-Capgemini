{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Word Embedding & Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal and outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our goal is to build a simple classification model and evaluate its performance when using the embedding matrices produced by three different embedding techniques: LSI and Word2Vec. \n",
    "\n",
    "In the three following sections we build the embedding matrices using each technique. Then, in section 4, we try to fit a cosine similarity classifier and a random forest classifier and evaluate their performances in predicting the ratings of the reviews as a target variable, with respect to the embeding matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:44:36.766753Z",
     "start_time": "2020-03-05T20:44:36.050988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os.path\n",
    "\n",
    "# Text manipulation\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP Modules\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, similarities\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Vizualisation\n",
    "import seaborn as sns\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use functions we defined in previous homework, running this cell takes few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:30.471684Z",
     "start_time": "2020-03-05T20:44:36.768280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[decid, visit, windsor, castl, way, back, sw, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[good, although, rather, small, portion, howev...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[look, somewher, budget, go, eat, overnight, w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[good, menu, select, unfortun, stifado, avail,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pop, last, night, glass, wine, attend, theatr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     stemmed_reviews  rating\n",
       "0  [decid, visit, windsor, castl, way, back, sw, ...       5\n",
       "1  [good, although, rather, small, portion, howev...       2\n",
       "2  [look, somewher, budget, go, eat, overnight, w...       5\n",
       "3  [good, menu, select, unfortun, stifado, avail,...       4\n",
       "4  [pop, last, night, glass, wine, attend, theatr...       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(DATA_PATH = \"data/\", file_name = \"raw_scrapped_data.csv.gzip\"):\n",
    "    \"\"\"\n",
    "    Input  : path of where data is stored\n",
    "    Purpose: loading csv file of reviews\n",
    "    Output : data frame of reviews with associated ratings\n",
    "    \"\"\"    \n",
    "    # Path of the file\n",
    "    file_path = DATA_PATH + file_name\n",
    "\n",
    "    # Reading data\n",
    "    scrapped_data = pd.read_csv(file_path, compression='gzip')\n",
    "    data = scrapped_data[['content', 'rating']]\n",
    "    return data \n",
    "\n",
    "def basic_cleaning(series):\n",
    "    # Remove punctuation\n",
    "    new_series = series.str.replace('[^\\w\\s]','')\n",
    "    # Strip trailing whitespace\n",
    "    new_series = new_series.str.strip(\" \")\n",
    "    # Decapitalize letters\n",
    "    new_series = new_series.apply(lambda x: str(x).lower())\n",
    "    return new_series\n",
    "\n",
    "def tokenize_filter(sentence):\n",
    "    # Define stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    ## Add personalised stop words\n",
    "    stop_words |= set([\"london\", \"food\", \"drink\", \"restaurant\"])\n",
    "    # Filter the sentence\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return (word_tokens, filtered_sentence)\n",
    "\n",
    "def stem_review(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    return tokens.apply(lambda x: [porter.stem(x[i]) for i in range(len(x))])\n",
    "\n",
    "def preprocess_data(data):\n",
    "    df = data\n",
    "    df[\"clean_content\"] = basic_cleaning(df[\"content\"])\n",
    "    df[\"tokenized_content\"] = df[\"clean_content\"].apply(lambda x: tokenize_filter(x)[1])\n",
    "    df[\"stemmed_reviews\"] = stem_review(df[\"tokenized_content\"])\n",
    "    return df[['stemmed_reviews', 'rating']]\n",
    "\n",
    "df = preprocess_data(load_data())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latent semantic indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the functions already defined in the handout and adapt them to our particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:30.476551Z",
     "start_time": "2020-03-05T20:45:30.473784Z"
    }
   },
   "outputs": [],
   "source": [
    "reduced_df = df[:1000] # We will use 1000 reviews for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Using \"corpora\" from gensim to extract vocabulary from a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:30.490015Z",
     "start_time": "2020-03-05T20:45:30.478204Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dictionary(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the whole associated vocabulary\n",
    "    Output : term dictionary\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our corpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    return corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Buildind TF matrix useful for LSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:30.501240Z",
     "start_time": "2020-03-05T20:45:30.491649Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_TF_matrix(doc_clean, useTransfertDict=True):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the term frequency matrix from a corpus\n",
    "    Output : Document Term Frequency Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our corpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "        \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    return [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Create an LSI model using Gensim and obtain LSI word embedding for our reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:32.574172Z",
     "start_time": "2020-03-05T20:45:30.502834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200)\n"
     ]
    }
   ],
   "source": [
    "def create_gensim_lsi_model(clean_documents_list, k=None):\n",
    "    \"\"\"\n",
    "    Input  : clean document, dictionary\n",
    "    Purpose: create LSI model (Latent Semantic Indexing) \n",
    "             from corpus and dictionary\n",
    "    Output : return LSI model\n",
    "    \"\"\"\n",
    "    \n",
    "    #LSI model consists of Singular Value Decomposition (SVD) of\n",
    "    #Term Document Matrix M: M = T x S x D'\n",
    "    #and dimensionality reductions of T, S and D (\"Derivation\")\n",
    "    \n",
    "    dictionary = get_dictionary(clean_documents_list)\n",
    "    \n",
    "    corpus = get_TF_matrix(clean_documents_list)\n",
    "    if k is not None:\n",
    "        lsi_model = LsiModel(\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=int(k)\n",
    "                )\n",
    "    else:\n",
    "            lsi_model = LsiModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary \n",
    "            )\n",
    "    #print(); print(); print(\"=\"*20, \"Training LSI model report\", \"=\"*20); print()\n",
    "    \n",
    "    #print(\"Initial TF matrix (NwordsXNdocuments): \")\n",
    "    TF = []\n",
    "    for x in corpus:\n",
    "        wrds = [0 for i in range(len(dictionary))]\n",
    "        for i, j in x: wrds[i] = j\n",
    "        TF.append(wrds)\n",
    "    #print(pd.np.transpose(TF))\n",
    "    #print()\n",
    "    #print(\"Derivation of Term Matrix T of Training Document Word Stems: \")\n",
    "    #print(lsi_model.get_topics())\n",
    "    #print()\n",
    "    #Derivation of Term Document Matrix of Training Document Word Stems = M' x [Derivation of T]\n",
    "    #print(\"LSI Vectors of 10 Training Document Word Stems: \")\n",
    "    #print([lsi_model[document_word_stems] for document_word_stems in corpus[:10]])\n",
    "    #print(\"=\"*70); print(); print()\n",
    "    return lsi_model\n",
    "\n",
    "def get_lsi_vector(lsi_model, clean_text, dictionary):\n",
    "    return lsi_model[dictionary.doc2bow(clean_text)]\n",
    "\n",
    "def get_lsi_matrix(lsi_model, corpus_TFmatrix):\n",
    "    return np.array(lsi_model[corpus_TFmatrix])[:,:,1]\n",
    "\n",
    "# create lsi model\n",
    "lsi_model = create_gensim_lsi_model(reduced_df.stemmed_reviews)\n",
    "# build encoded corpus (TF matrix)\n",
    "corpus_TFmatrix = get_TF_matrix(reduced_df.stemmed_reviews)\n",
    "# obtain the LSI representation of our reviews in the form of a matrix\n",
    "lsi_matrix = get_lsi_matrix(lsi_model, corpus_TFmatrix)\n",
    "print(lsi_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Instantiating the model from gensim models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:32.580298Z",
     "start_time": "2020-03-05T20:45:32.576347Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_word2vec_model():\n",
    "    \"\"\"\n",
    "    Input  : none\n",
    "    Purpose: create word2vec model from corpus\n",
    "    Output : term dictionary\n",
    "    \"\"\"\n",
    "    path = get_tmpfile(\"word2vec.model\")\n",
    "    model = gensim.models.Word2Vec(size=300, \n",
    "                                   window=3, \n",
    "                                   min_count=5, \n",
    "                                   workers=4, \n",
    "                                   seed=1, \n",
    "                                   iter=50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Building model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:32.596125Z",
     "start_time": "2020-03-05T20:45:32.582012Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_vocab(model, clean_documents_list):\n",
    "    \"\"\"\n",
    "    Input  : model and clean documents list\n",
    "    Purpose: instantiate model vocabulary from clean documents list\n",
    "    Output : model with vocabulary\n",
    "    \"\"\"\n",
    "    init_vocab = list(map(lambda review: review, clean_documents_list[\"stemmed_reviews\"]))\n",
    "    model.build_vocab(init_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Training the model on the reviews, then saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:32.601860Z",
     "start_time": "2020-03-05T20:45:32.597496Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_word2vec_model(model, clean_documents_list):\n",
    "    \"\"\"\n",
    "    Input  : model and clean documents list\n",
    "    Purpose: train model on clean documents list\n",
    "    Output : trained model\n",
    "    \"\"\"\n",
    "    corpus = list(map(lambda review: review, clean_documents_list[\"stemmed_reviews\"]))\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save(\"word2vec.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Loading pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:32.607632Z",
     "start_time": "2020-03-05T20:45:32.603014Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_trained_model(model_name):\n",
    "    \"\"\"\n",
    "    Input  : trained model name\n",
    "    Purpose: load trained model\n",
    "    Output : saved model\n",
    "    \"\"\"\n",
    "    return gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Getting embedded matrix from corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:32.613838Z",
     "start_time": "2020-03-05T20:45:32.609116Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word2vec_matrix(model):\n",
    "    embedding_matrix = dict()\n",
    "    for word in model.wv.vocab.keys():\n",
    "        embedding_matrix[word] = list(model.wv[word])\n",
    "    return pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating and training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:34.386102Z",
     "start_time": "2020-03-05T20:45:32.614988Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('superb', 0.7938962578773499),\n",
       " ('excel', 0.756244957447052),\n",
       " ('outstand', 0.7333143353462219),\n",
       " ('throughout', 0.7261067628860474),\n",
       " ('buzz', 0.716883659362793),\n",
       " ('love', 0.6926382780075073),\n",
       " ('impecc', 0.6919640302658081),\n",
       " ('good', 0.6886517405509949),\n",
       " ('faultless', 0.68587726354599),\n",
       " ('brilliant', 0.6840408444404602)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_word2vec_model()\n",
    "model = init_vocab(model, reduced_df)\n",
    "model = train_word2vec_model(model, reduced_df)\n",
    "model = load_trained_model(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"great\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving embedding matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:34.628851Z",
     "start_time": "2020-03-05T20:45:34.388121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1278)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decid</th>\n",
       "      <th>visit</th>\n",
       "      <th>way</th>\n",
       "      <th>back</th>\n",
       "      <th>england</th>\n",
       "      <th>saw</th>\n",
       "      <th>establish</th>\n",
       "      <th>english</th>\n",
       "      <th>beer</th>\n",
       "      <th>thought</th>\n",
       "      <th>...</th>\n",
       "      <th>appetit</th>\n",
       "      <th>broken</th>\n",
       "      <th>pipe</th>\n",
       "      <th>45</th>\n",
       "      <th>court</th>\n",
       "      <th>dairi</th>\n",
       "      <th>sorbet</th>\n",
       "      <th>mother</th>\n",
       "      <th>effort</th>\n",
       "      <th>aunt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.323729</td>\n",
       "      <td>0.139122</td>\n",
       "      <td>-0.117710</td>\n",
       "      <td>-0.362096</td>\n",
       "      <td>-0.097604</td>\n",
       "      <td>-0.113042</td>\n",
       "      <td>-0.170597</td>\n",
       "      <td>-0.191632</td>\n",
       "      <td>-0.074101</td>\n",
       "      <td>0.142660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248556</td>\n",
       "      <td>-0.150945</td>\n",
       "      <td>-0.252921</td>\n",
       "      <td>-0.245601</td>\n",
       "      <td>-0.113153</td>\n",
       "      <td>-0.063010</td>\n",
       "      <td>-0.255565</td>\n",
       "      <td>-0.213441</td>\n",
       "      <td>-0.214218</td>\n",
       "      <td>-0.175309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019365</td>\n",
       "      <td>-0.139541</td>\n",
       "      <td>-0.184012</td>\n",
       "      <td>-0.334717</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>-0.007540</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>0.126799</td>\n",
       "      <td>0.200532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066770</td>\n",
       "      <td>0.123345</td>\n",
       "      <td>0.126601</td>\n",
       "      <td>0.100687</td>\n",
       "      <td>0.015741</td>\n",
       "      <td>0.044488</td>\n",
       "      <td>0.172349</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>0.029864</td>\n",
       "      <td>0.075839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.033673</td>\n",
       "      <td>-0.106202</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.010697</td>\n",
       "      <td>-0.060481</td>\n",
       "      <td>-0.066420</td>\n",
       "      <td>-0.230945</td>\n",
       "      <td>0.074824</td>\n",
       "      <td>-0.284938</td>\n",
       "      <td>-0.388685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029950</td>\n",
       "      <td>-0.098206</td>\n",
       "      <td>-0.014048</td>\n",
       "      <td>-0.097435</td>\n",
       "      <td>-0.072245</td>\n",
       "      <td>-0.153745</td>\n",
       "      <td>-0.006930</td>\n",
       "      <td>0.008833</td>\n",
       "      <td>-0.062296</td>\n",
       "      <td>-0.083426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.249941</td>\n",
       "      <td>0.699008</td>\n",
       "      <td>-0.216362</td>\n",
       "      <td>-0.066214</td>\n",
       "      <td>-0.004977</td>\n",
       "      <td>-0.001074</td>\n",
       "      <td>-0.041546</td>\n",
       "      <td>-0.224951</td>\n",
       "      <td>-0.515194</td>\n",
       "      <td>-0.311595</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189921</td>\n",
       "      <td>-0.166494</td>\n",
       "      <td>-0.143823</td>\n",
       "      <td>-0.098051</td>\n",
       "      <td>-0.029900</td>\n",
       "      <td>-0.122641</td>\n",
       "      <td>-0.247966</td>\n",
       "      <td>-0.009082</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>-0.167865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.270207</td>\n",
       "      <td>-0.196001</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>0.607687</td>\n",
       "      <td>0.251987</td>\n",
       "      <td>-0.003024</td>\n",
       "      <td>0.344423</td>\n",
       "      <td>0.216505</td>\n",
       "      <td>0.289487</td>\n",
       "      <td>0.075034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271758</td>\n",
       "      <td>0.069775</td>\n",
       "      <td>0.201372</td>\n",
       "      <td>0.111284</td>\n",
       "      <td>0.113445</td>\n",
       "      <td>0.010921</td>\n",
       "      <td>0.161079</td>\n",
       "      <td>0.166344</td>\n",
       "      <td>0.136033</td>\n",
       "      <td>0.041274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      decid     visit       way      back   england       saw  establish  \\\n",
       "0 -0.323729  0.139122 -0.117710 -0.362096 -0.097604 -0.113042  -0.170597   \n",
       "1  0.019365 -0.139541 -0.184012 -0.334717  0.065058  0.004493  -0.007540   \n",
       "2 -0.033673 -0.106202  0.016617  0.010697 -0.060481 -0.066420  -0.230945   \n",
       "3 -0.249941  0.699008 -0.216362 -0.066214 -0.004977 -0.001074  -0.041546   \n",
       "4  0.270207 -0.196001  0.018785  0.607687  0.251987 -0.003024   0.344423   \n",
       "\n",
       "    english      beer   thought  ...   appetit    broken      pipe        45  \\\n",
       "0 -0.191632 -0.074101  0.142660  ... -0.248556 -0.150945 -0.252921 -0.245601   \n",
       "1  0.005203  0.126799  0.200532  ...  0.066770  0.123345  0.126601  0.100687   \n",
       "2  0.074824 -0.284938 -0.388685  ... -0.029950 -0.098206 -0.014048 -0.097435   \n",
       "3 -0.224951 -0.515194 -0.311595  ... -0.189921 -0.166494 -0.143823 -0.098051   \n",
       "4  0.216505  0.289487  0.075034  ...  0.271758  0.069775  0.201372  0.111284   \n",
       "\n",
       "      court     dairi    sorbet    mother    effort      aunt  \n",
       "0 -0.113153 -0.063010 -0.255565 -0.213441 -0.214218 -0.175309  \n",
       "1  0.015741  0.044488  0.172349  0.007982  0.029864  0.075839  \n",
       "2 -0.072245 -0.153745 -0.006930  0.008833 -0.062296 -0.083426  \n",
       "3 -0.029900 -0.122641 -0.247966 -0.009082  0.000888 -0.167865  \n",
       "4  0.113445  0.010921  0.161079  0.166344  0.136033  0.041274  \n",
       "\n",
       "[5 rows x 1278 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_word2vecMatrix = get_word2vec_matrix(model)\n",
    "print(corpus_word2vecMatrix.shape)\n",
    "corpus_word2vecMatrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of the performance of classifiers on different types of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we split train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:34.740379Z",
     "start_time": "2020-03-05T20:45:34.630984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train/test:  (666, 2) VS (334, 2)\n"
     ]
    }
   ],
   "source": [
    "df_dataset = reduced_df\n",
    "n = len(df_dataset)\n",
    "df_dataset.sample(n=n, random_state=16)\n",
    "n = int(2 * n / 3)\n",
    "df_dataset_train = df_dataset[:n]\n",
    "df_dataset_test = df_dataset[n:]\n",
    "print(\"Split train/test: \", df_dataset_train.shape, \"VS\", df_dataset_test.shape)\n",
    "corpus_TFmatrix_train = get_TF_matrix(df_dataset_train.stemmed_reviews)\n",
    "corpus_TFmatrix_test = get_TF_matrix(df_dataset_test.stemmed_reviews)\n",
    "y_train=df_dataset_train.rating\n",
    "y_test=df_dataset_test.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the classifier defined in the handout on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:34.959190Z",
     "start_time": "2020-03-05T20:45:34.741635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x7f07deb36d68>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance_classifier_cosine_traning(lsi_vector_trainDB):\n",
    "    \"\"\"\n",
    "    Input  : LSI vectors\n",
    "    Purpose: calculate cosine similarity matrix\n",
    "    Output : return similarity matrix\n",
    "    \"\"\"\n",
    "    #calculate cosine similarity matrix for all training document LSI vectors\n",
    "    return similarities.MatrixSimilarity(lsi_vector_trainDB)\n",
    "\n",
    "def distance_classifier_cosine_test(classification_model, training_data, test_doc_lsi_vector, N=1):\n",
    "    \"\"\"\n",
    "    Input  : trained classifier model, the training data (list of descriptions), lsi vectors of a document and N nearest document in the training data base\n",
    "    Purpose: calculate cosine similarity matrix against all training samples\n",
    "    Output : return nearest N document and classes\n",
    "    \"\"\"\n",
    "    cosine_similarities = classification_model[test_doc_lsi_vector]\n",
    "\n",
    "    most_similar_document_test = training_data[np.argmax(cosine_similarities)]\n",
    "\n",
    "    #calculate cosine similarity matrix for all training document LSI vectors\n",
    "    return most_similar_document_test\n",
    "\n",
    "def reco_rate(ref_labels, predicted_labels):\n",
    "    commun_labels = (pd.np.array(ref_labels)==pd.np.array(predicted_labels)).sum()\n",
    "    return 100 * commun_labels / len(ref_labels)\n",
    "\n",
    "classification_model = distance_classifier_cosine_traning(lsi_model[corpus_TFmatrix_train])\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test on train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:36.086957Z",
     "start_time": "2020-03-05T20:45:34.960573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:26: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "dictionary = get_dictionary(df_dataset.stemmed_reviews)\n",
    "predicted_ratings = [distance_classifier_cosine_test(classification_model, \n",
    "                                df_dataset_train.rating, \n",
    "                                get_lsi_vector(lsi_model, df_dataset_train.stemmed_reviews.iloc[i]))\n",
    "                                for i in range(df_dataset_train.shape[0])]\n",
    "\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprinsingly, the cosine distance classifier performs perfectly on test data because it by definition the distance of each stemmed review to itself is zero and the decision is easily made. Let's see how it performs on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:36.813517Z",
     "start_time": "2020-03-05T20:45:36.089129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on test DB: 50.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:26: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "predicted_ratings_test = [distance_classifier_cosine_test(classification_model, \n",
    "                                 df_dataset_train.rating, \n",
    "                                 get_lsi_vector(lsi_model, \n",
    "                                               df_dataset_test.stemmed_reviews.iloc[i]\n",
    "                                              ))\n",
    "                   for i in range(df_dataset_test.shape[0])]\n",
    "\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the test reviews and their ratings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:36.839399Z",
     "start_time": "2020-03-05T20:45:36.815726Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>[pub, great, quit, cheap, consid, staff, atten...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>[best, thai, eaten, lobster, green, curri, fan...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>[one, arriv, love, place, understand, one, nic...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>[welcom, mouth, water, smell, steak, enter, ga...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>[went, waffl, jack, wretch, hangov, love, waff...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>[husband, decid, go, winter, garden, anniversa...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>[say, enough, excel, gripe, sauc, turbot, stro...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>[book, birthday, presentup, moment, didnt, kno...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>[good, surpris, went, curiou, sinc, friend, mi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>[burger, delici, best, valu, price, eat, burge...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       stemmed_reviews  rating\n",
       "666  [pub, great, quit, cheap, consid, staff, atten...       3\n",
       "667  [best, thai, eaten, lobster, green, curri, fan...       5\n",
       "668  [one, arriv, love, place, understand, one, nic...       5\n",
       "669  [welcom, mouth, water, smell, steak, enter, ga...       5\n",
       "670  [went, waffl, jack, wretch, hangov, love, waff...       5\n",
       "671  [husband, decid, go, winter, garden, anniversa...       5\n",
       "672  [say, enough, excel, gripe, sauc, turbot, stro...       5\n",
       "673  [book, birthday, presentup, moment, didnt, kno...       4\n",
       "674  [good, surpris, went, curiou, sinc, friend, mi...       4\n",
       "675  [burger, delici, best, valu, price, eat, burge...       4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:45:36.846691Z",
     "start_time": "2020-03-05T20:45:36.841664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 4, 5, 5, 5, 5, 4, 5, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ratings_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the cosine classifier performs very poorly on test data using LSI embedding, we switch to random forest classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:47:46.700427Z",
     "start_time": "2020-03-05T20:47:46.697335Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:47:48.033277Z",
     "start_time": "2020-03-05T20:47:48.023783Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_training(clf,X_train,y_train):\n",
    "    \"\"\"\n",
    "    Input  : LSI vectors\n",
    "    Purpose: train classification model\n",
    "    Output : return trained classifier\n",
    "    \"\"\"\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf\n",
    "\n",
    "def model_predictions(clf, X_test):\n",
    "    \"\"\"\n",
    "    Input  : trained classifier model, the test set\n",
    "    Purpose: make predictions on test data\n",
    "    Output : predictions\n",
    "    \"\"\"\n",
    "    y_pred=clf.predict(X_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def reco_rate(ref_labels, predicted_labels):\n",
    "    commun_labels = (pd.np.array(ref_labels)==pd.np.array(predicted_labels)).sum()\n",
    "    return 100 * commun_labels / len(ref_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters to tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:47:50.711852Z",
     "start_time": "2020-03-05T20:47:50.685293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the random grid to search for best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T21:14:49.404949Z",
     "start_time": "2020-03-05T21:10:02.857214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=41, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=41, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(get_lsi_matrix(lsi_model, corpus_TFmatrix_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T21:14:49.410350Z",
     "start_time": "2020-03-05T21:14:49.406802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 100,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T21:19:08.289737Z",
     "start_time": "2020-03-05T21:19:07.900679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 100.00 %\n",
      "Classifier performances on test DB: 53.59 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "# We test on train data\n",
    "predicted_ratings = model_predictions(rf_random, get_lsi_matrix(lsi_model, corpus_TFmatrix_train))\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")\n",
    "\n",
    "# We test on test Data\n",
    "predicted_ratings_test = model_predictions(clf, get_lsi_matrix(lsi_model, corpus_TFmatrix_test))\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:50:16.337751Z",
     "start_time": "2020-03-05T20:50:16.333543Z"
    }
   },
   "outputs": [],
   "source": [
    "def document_vector(model, doc):\n",
    "    ##remove words that aren't in vocabulary\n",
    "    doc = [word for word in doc if word in model.wv.vocab.keys()]\n",
    "    return np.sum(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:50:17.810766Z",
     "start_time": "2020-03-05T20:50:17.666766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for doc in df_dataset_train.stemmed_reviews: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "X_train = np.array(x)\n",
    "\n",
    "x = []\n",
    "for doc in df_dataset_test.stemmed_reviews: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "X_test = np.array(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the random grid to search for best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:56:59.225867Z",
     "start_time": "2020-03-05T20:50:31.292490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  6.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T20:56:59.230762Z",
     "start_time": "2020-03-05T20:56:59.227304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 90,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-05T21:06:23.404760Z",
     "start_time": "2020-03-05T21:06:23.323284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 100.00 %\n",
      "Classifier performances on test DB: 58.08 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "/home/leonardo/Documents/University/X/Courses/Cap/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n"
     ]
    }
   ],
   "source": [
    "# We test on train data\n",
    "predicted_ratings = model_predictions(rf_random, X_train)\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")\n",
    "\n",
    "# We test on test Data\n",
    "predicted_ratings_test = model_predictions(rf_random, X_test)\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our tuned Random Forests do not perfom well, we believe this is due to the reduced number of reviews (1000) we are using. Increasing the number of reviews and trying to tune RFs on those reviews requires computing power that our machines can't handle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
