{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Word Embedding & Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal and outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, our goal is to build a simple classification model and evaluate its performance when using the embedding matrices produced by three different embedding techniques: LSI, Word2Vec and FastText. \n",
    "\n",
    "In the three following sections we build the embedding matrices using each technique. Then, in section 4, we try to fit a cosine similarity classifier and a random forest classifier and evaluate its performances in predicting the ratings of the reviews as a target variable, with respect to the embeding matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os.path\n",
    "\n",
    "# Text manipulation\n",
    "from string import punctuation\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP Modules\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, similarities\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import LsiModel\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Vizualisation\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "\n",
    "# Extra imports\n",
    "\n",
    "# Uncomment the following lines if you haven't installed gensim and nltk\n",
    "#!pip3 install gensim\n",
    "#!pip3 install nltk\n",
    "\n",
    "# Downloading useful nltk packages if not already done\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use functions we defined in previous homework, running this cell takes few minutes\n",
    "\n",
    "def load_data(DATA_PATH = \"data/\", file_name = \"raw_scrapped_data.csv.gzip\"):\n",
    "    \"\"\"\n",
    "    Input  : path of where data is stored\n",
    "    Purpose: loading csv file of reviews\n",
    "    Output : data frame of reviews with associated ratings\n",
    "    \"\"\"    \n",
    "    # Path of the file\n",
    "    file_path = DATA_PATH + file_name\n",
    "\n",
    "    # Reading data\n",
    "    scrapped_data = pd.read_csv(file_path, compression='gzip')\n",
    "    data = scrapped_data[['content', 'rating']]\n",
    "    return data \n",
    "\n",
    "def basic_cleaning(series):\n",
    "    # Remove punctuation\n",
    "    new_series = series.str.replace('[^\\w\\s]','')\n",
    "    # Strip trailing whitespace\n",
    "    new_series = new_series.str.strip(\" \")\n",
    "    # Decapitalize letters\n",
    "    new_series = new_series.apply(lambda x: str(x).lower())\n",
    "    return new_series\n",
    "\n",
    "def tokenize_filter(sentence):\n",
    "    # Define stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    ## Add personalised stop words\n",
    "    stop_words |= set([\"london\", \"food\", \"drink\", \"restaurant\"])\n",
    "    # Filter the sentence\n",
    "    word_tokens = word_tokenize(sentence) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return (word_tokens, filtered_sentence)\n",
    "\n",
    "def stem_review(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    return tokens.apply(lambda x: [porter.stem(x[i]) for i in range(len(x))])\n",
    "\n",
    "def preprocess_data(data):\n",
    "    df = data\n",
    "    df[\"clean_content\"] = basic_cleaning(df[\"content\"])\n",
    "    df[\"tokenized_content\"] = df[\"clean_content\"].apply(lambda x: tokenize_filter(x)[1])\n",
    "    df[\"stemmed_reviews\"] = stem_review(df[\"tokenized_content\"])\n",
    "    return df[['stemmed_reviews', 'rating']]\n",
    "\n",
    "df = preprocess_data(load_data())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Latent semantic indexing (LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the functions already defined in the handout and adapt them to our particular case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 100 reviews for now\n",
    "reduced_df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using \"corpora\" from gensim to extract vocabulary from a corpus\n",
    "def get_dictionary(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the whole associated vocabulary\n",
    "    Output : term dictionary\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    return corpora.Dictionary(doc_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Buildind TF matrix useful for LSI\n",
    "def get_TF_matrix(doc_clean, useTransfertDict=True):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the term frequency matrix from a corpus\n",
    "    Output : Document Term Frequency Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "        \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    return [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create an LSI model using Gensim\n",
    "def create_gensim_lsi_model(clean_documents_list, k=None):\n",
    "    \"\"\"\n",
    "    Input  : clean document, dictionary\n",
    "    Purpose: create LSI model (Latent Semantic Indexing) \n",
    "             from corpus and dictionary\n",
    "    Output : return LSI model\n",
    "    \"\"\"\n",
    "    \n",
    "    #LSI model consists of Singular Value Decomposition (SVD) of\n",
    "    #Term Document Matrix M: M = T x S x D'\n",
    "    #and dimensionality reductions of T, S and D (\"Derivation\")\n",
    "    \n",
    "    dictionary = get_dictionary(clean_documents_list)\n",
    "    \n",
    "    corpus = get_TF_matrix(clean_documents_list)\n",
    "    if k is not None:\n",
    "        lsi_model = LsiModel(\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=int(k)\n",
    "                )\n",
    "    else:\n",
    "            lsi_model = LsiModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary \n",
    "            )\n",
    "    print(); print(); print(\"=\"*20, \"Training LSI model report\", \"=\"*20); print()\n",
    "    \n",
    "    print(\"Initial TF matrix (NwordsXNdocuments): \")\n",
    "    TF = []\n",
    "    for x in corpus:\n",
    "        wrds = [0 for i in range(len(dictionary))]\n",
    "        for i, j in x: wrds[i] = j\n",
    "        TF.append(wrds)\n",
    "    print(pd.np.transpose(TF))\n",
    "    print()\n",
    "    print(\"Derivation of Term Matrix T of Training Document Word Stems: \")\n",
    "    print(lsi_model.get_topics())\n",
    "    print()\n",
    "    #Derivation of Term Document Matrix of Training Document Word Stems = M' x [Derivation of T]\n",
    "    print(\"LSI Vectors of Training Document Word Stems: \")\n",
    "    print([lsi_model[document_word_stems] for document_word_stems in corpus])\n",
    "    print(\"=\"*70); print(); print()\n",
    "    return lsi_model\n",
    "\n",
    "def get_lsi_vector(lsi_model, clean_text):\n",
    "    return lsi_model[dictionary.doc2bow(clean_text)]\n",
    "\n",
    "# create lsi model\n",
    "lsi_model = create_gensim_lsi_model(reduced_df.stemmed_reviews)\n",
    "# build encoded corpus (TF matrix)\n",
    "corpus_TFmatrix = get_TF_matrix(reduced_df.stemmed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiating the model from gensim models\n",
    "def create_word2vec_model():\n",
    "    \"\"\"\n",
    "    Input  : none\n",
    "    Purpose: create word2vec model from corpus\n",
    "    Output : term dictionary\n",
    "    \"\"\"\n",
    "    path = get_tmpfile(\"word2vec.model\")\n",
    "    model = gensim.models.Word2Vec(size=300, \n",
    "                                   window=3, \n",
    "                                   min_count=5, \n",
    "                                   workers=4, \n",
    "                                   seed=1, \n",
    "                                   iter=50)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Building model vocabulary\n",
    "def init_vocab(model, clean_documents_list):\n",
    "    \"\"\"\n",
    "    Input  : model and clean documents list\n",
    "    Purpose: instantiate model vocabulary from clean documents list\n",
    "    Output : model with vocabulary\n",
    "    \"\"\"\n",
    "    init_vocab = list(map(lambda review: review, clean_documents_list[\"stemmed_reviews\"]))\n",
    "    model.build_vocab(init_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training the model on the reviews, then saving it\n",
    "def train_word2vec_model(model, clean_documents_list):\n",
    "    \"\"\"\n",
    "    Input  : model and clean documents list\n",
    "    Purpose: train model on clean documents list\n",
    "    Output : trained model\n",
    "    \"\"\"\n",
    "    corpus = list(map(lambda review: review, clean_documents_list[\"stemmed_reviews\"]))\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save(\"word2vec.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loading pre-trained model\n",
    "def load_trained_model(model_name):\n",
    "    \"\"\"\n",
    "    Input  : trained model name\n",
    "    Purpose: load trained model\n",
    "    Output : saved model\n",
    "    \"\"\"\n",
    "    return gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedded_matrix(model):\n",
    "    embedding_matrix = dict()\n",
    "    for word in model.wv.vocab.keys():\n",
    "    embedding_matrix[word] = list(model.wv[word])\n",
    "    return pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_word2vec_model()\n",
    "model = init_vocab(model, reduced_df)\n",
    "model = train_word2vec_model(model, reduced_df)\n",
    "model = load_trained_model(\"word2vec.model\")\n",
    "# Testing for an example\n",
    "model.most_similar(\"great\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison of the performance of a Random Forest on different types of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train/test:  (66, 2) VS (34, 2)\n"
     ]
    }
   ],
   "source": [
    "# First we split train/test data\n",
    "\n",
    "df_dataset = reduced_df\n",
    "n = len(df_dataset)\n",
    "df_dataset.sample(n=n, random_state=16)\n",
    "n = int(2 * n / 3)\n",
    "df_dataset_train = df_dataset[:n]\n",
    "df_dataset_test = df_dataset[n:]\n",
    "print(\"Split train/test: \", df_dataset_train.shape, \"VS\", df_dataset_test.shape)\n",
    "corpus_TFmatrix_train = get_TF_matrix(df_dataset_train.stemmed_reviews)\n",
    "corpus_TFmatrix_test = get_TF_matrix(df_dataset_test.stemmed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x1a4ede76a0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we use the classifier defined in the handout on our dataset\n",
    "\n",
    "def distance_classifier_cosine_traning(lsi_vector_trainDB):\n",
    "    \"\"\"\n",
    "    Input  : LSI vectors\n",
    "    Purpose: calculate cosine similarity matrix\n",
    "    Output : return similarity matrix\n",
    "    \"\"\"\n",
    "    #calculate cosine similarity matrix for all training document LSI vectors\n",
    "    return similarities.MatrixSimilarity(lsi_vector_trainDB)\n",
    "\n",
    "def distance_classifier_cosine_test(classification_model, training_data, test_doc_lsi_vector, N=1):\n",
    "    \"\"\"\n",
    "    Input  : trained classifier model, the training data (list of descriptions), lsi vectors of a document and N nearest document in the training data base\n",
    "    Purpose: calculate cosine similarity matrix against all training samples\n",
    "    Output : return nearest N document and classes\n",
    "    \"\"\"\n",
    "    cosine_similarities = classification_model[test_doc_lsi_vector]\n",
    "\n",
    "    most_similar_document_test = training_data[np.argmax(cosine_similarities)]\n",
    "\n",
    "    #calculate cosine similarity matrix for all training document LSI vectors\n",
    "    return most_similar_document_test\n",
    "\n",
    "def reco_rate(ref_labels, predicted_labels):\n",
    "    commun_labels = (pd.np.array(ref_labels)==pd.np.array(predicted_labels)).sum()\n",
    "    return 100 * commun_labels / len(ref_labels)\n",
    "\n",
    "classification_model = distance_classifier_cosine_traning(lsi_model[corpus_TFmatrix_train])\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on train DB: 100.00 %\n"
     ]
    }
   ],
   "source": [
    "# We test on train data\n",
    "dictionary = get_dictionary(df_dataset.stemmed_reviews)\n",
    "predicted_ratings = [distance_classifier_cosine_test(classification_model, \n",
    "                                df_dataset_train.rating, \n",
    "                                get_lsi_vector(lsi_model, df_dataset_train.stemmed_reviews.iloc[i]))\n",
    "                                for i in range(df_dataset_train.shape[0])]\n",
    "\n",
    "print(\"Classifier performances on train DB: %.2f\" % reco_rate(df_dataset_train.rating, predicted_ratings), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprinsingly, the cosine distance classifier performs perfectly on test data because it by definition the distance of each stemmed review to itself is zero and the decision is easily made. Let's see how it performs on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier performances on test DB: 41.18 %\n"
     ]
    }
   ],
   "source": [
    "# We test on test Data\n",
    "predicted_ratings_test = [distance_classifier_cosine_test(classification_model, \n",
    "                                 df_dataset_train.rating, \n",
    "                                 get_lsi_vector(lsi_model, \n",
    "                                               df_dataset_test.stemmed_reviews.iloc[i]\n",
    "                                              ))\n",
    "                   for i in range(df_dataset_test.shape[0])]\n",
    "\n",
    "print(\"Classifier performances on test DB: %.2f\"%(reco_rate(df_dataset_test.rating, predicted_ratings_test)), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_reviews</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>[select, lunch, south, african, busi, guest, c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>[miss, love, indonesian, cuisin, holiday, hop,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>[great, good, wine, list, attent, courteou, se...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>[friend, went, weekend, absolut, love, believ,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[first, time, visit, sceneri, took, breath, aw...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[love, pint, beer, littl, els, call, windsor, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[spanish, tapa, best, great, locat, realli, go...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>[time, lunch, work, peopl, last, time, worst, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>[stay, close, good, littl, place, good, locat,...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[got, introduc, friend, hook, triangl, love, h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      stemmed_reviews  rating\n",
       "66  [select, lunch, south, african, busi, guest, c...       4\n",
       "67  [miss, love, indonesian, cuisin, holiday, hop,...       4\n",
       "68  [great, good, wine, list, attent, courteou, se...       5\n",
       "69  [friend, went, weekend, absolut, love, believ,...       4\n",
       "70  [first, time, visit, sceneri, took, breath, aw...       4\n",
       "71  [love, pint, beer, littl, els, call, windsor, ...       3\n",
       "72  [spanish, tapa, best, great, locat, realli, go...       5\n",
       "73  [time, lunch, work, peopl, last, time, worst, ...       3\n",
       "74  [stay, close, good, littl, place, good, locat,...       4\n",
       "75  [got, introduc, friend, hook, triangl, love, h...       5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take a look at the test reviews and their ratings \n",
    "df_dataset_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 3, 5, 3, 5, 2, 5, 5]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is our prediction\n",
    "predicted_ratings_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the cosine classifier performs very poorly on test data using LSI embedding, we switch to random forest classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
